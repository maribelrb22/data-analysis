---
title: "Mushroom Data Analysis"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---
<style>
body {
text-align: justify}
</style>

# Índice   
1. [Introducción](#introduction) \
2. [Cargar librerías](#libraries) \
3. [Cargar datos](#data) \
4. [Visualización y preprocesamiento](#preprocessing) \
4.1 [Imputación de valores nulos](#imputation) \
4.2 [Codificación de variables categóricas](#encoding) \
4.3 [Escalado de variables numéricas](#scaling)
5. [Análisis de datos](#analysis) \
5.1 [Análisis aprendizaje supervisado](#supervised) \
5.2 [Análisis aprendizaje no supervisado](#unsupervised) \
5.2.1 [K-means](#kmeans) \
5.2.2 [Clustering jerárquico](#hierarchical) \

\pagebreak

# Introducción<a name="introduction"></a>

Este trabajo se centra en el análisis de un conjunto de datos que contiene información sobre 61069 champiñones diferentes. Los datos han sido obtenidos de Kaggle y cada una de las instancias incluye 21 variables que describen diferentes aspectos de los champiñones, como su forma, tamaño y hábitat. En estas variables se encuentra inlcuida la clase de cada champiñón, indicando si es venenoso o comestible.

Antes de comenzar el análisis, es necesario realizar una fase de preprocesamiento de los datos. Esta fase tiene como objetivo preparar los datos para su análisis y obtener mayor conocimiento sobre estos. Durante esta fase, realizaremos tareas como visualizar los datos, detectar y tratar valores faltantes, o normalizar los datos.
Una vez preparados los datos, procederemos a realizar el análisis. Para ello, utilizaremos tanto técnicas de aprendizaje supervisado como no supervisado.

El aprendizaje supervisado implica entrenar un modelo con datos etiquetados, es decir, que ya conocemos la clase de cada instancia. Una vez entrenado el modelo, podemos utilizarlo para predecir la clase de nuevas instancias. En este caso, utilizaremos diferentes algoritmos de clasificación para evaluar diferentes modelos.

El aprendizaje no supervisado, por otro lado, no requiere de datos etiquetados. En este caso, el objetivo es agrupar las instancias en diferentes grupos de forma que las instancias de un mismo grupo sean similares entre sí y diferentes a las de los demás grupos. Utilizaremos dos algoritmos de clustering para evaluar diferentes modelos. Además, cabe destacar, que en nuestro caso contamos con un conocimiento a priori de los datos, ya que conocemos la clase de cada instancia. Por tanto, podemos utilizar este conocimiento para evaluar los modelos de clustering.

A continuación se detalla los posibles valores que pueden tomar las variables del dataset:

**1. class:** edible=e, poisonous=p \
**2. cap-diameter:** float number in cm \
**3. cap-shape:** bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s \
**4. cap-surface:** fibrous=f, grooves=g, scaly=y, smooth=s \
**5. cap-color:** brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y \
**6. does.bruise.or.bleed:** bruises-or-bleeding=t,no=f \
**7. gill-attachment:** attached=a, descending=d, free=f, notched=n \
**8. gill-spacing:** close=c, crowded=w, distant=d \
**9. gill-color:** black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \
**10. stem-height:** float number in cm \
**11. stem-width:** float number in mm  \
**12. stem-root:** bulbous=b, swollen=s, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r \
**13. stem-surface:** see cap-surface + none=f \
**14. stem-color:** see cap-color + none=f \
**15. veil-type:** partial=p, universal=u \
**16. veil-color:** see cap-color + none=f \
**17. has-ring:** ring=t, none=f \
**18. ring-type:** cobwebby=c, evanescent=e, flaring=r, grooved=g, large=l, pendant=p, sheathing=s, zone=z, scaly=y, movable=m, none=f, unknown=? \
**19. spore-print-color:** see cap color \
**20. habitat:** grasses=g, leaves=l, meadows=m, paths=p, heaths=h, urban=u, waste=w, woods=d \
**21. season:** spring=s, summer=u, autumn=a, winter=w \

# Autores<a name="authors"></a>
Este trabajo ha sido realizado por:

* Ana Díaz Muñoz

* María Isabel Ramos Blanco

* Deyan Rosenov Stanchev

* Javier Vilariño Mayo


Todos los integrantes del grupo han realizado de forma conjunta la visualización y el preprocesamiento de los datos. Ana y Deyan se ha encargado de realizar el aprendizaje supervisado, mientras que Javier y María Isabel se han encargado de realizar el aprendizaje no supervisado.
Por otro lado, todos los componentes han investigado acerca del clustering utilizado en BigML.


# Cargar librerías<a name="libraries"></a>

En los siguientes fragmentos se incluye el código necesario para instalar y cargar las librerías necesarias para el análisis de los datos realizado.
```{r}
#install.packages("caret")
#install.packages("tidyverse")
#install.packages("plotly")
#install.packages("dplyr")
#install.packages("factoextra")
#install.packages("dendextend")
```
```{r}
library(caret)
library(tidyverse)
library(plotly)
library(dplyr)
library(cluster)
library(factoextra)
library(dendextend)
```

# Cargar datos<a name="data"></a>

En primer lugar, realizamos la lectura del fichero csv, separando las columnas por ";" y mostramos las primeras 6 filas del fichero. Este código nos permitirá acceder a los datos de champiñones y trabajar con ellos en nuestro código R.
```{r}
mushroom <- read.csv("./data/data.csv", sep = ";")
head(mushroom)
```

Echamos un vistazo a las características de los atributos del dataset. En el caso de las variables numéricas, se puede observar valores como el mínimo, máximo, media, desviación estándar, etc. Por otro lado, en el caso de las variables categóricas no obtenemos información relevante.
```{r}
summary(mushroom)
```

# Visualización y preprocesamiento<a name="preprocessing"></a>

En primer lugar, vamos a comprobar si existen valores nulos en el dataset. Para ello, utilizaremos la función colSums(is.na(mushroom)), que nos devolverá la suma de valores nulos de cada variable.
```{r}
colSums(is.na(mushroom))
```

Según el resultado obtenido anteriormente, no existen valores nulos en el dataset. Sin embargo, si observamos el dataset, podemos observar que existen valores vacíos. Para poder trabajar con ellos, y que no nos de problemas a la hora de realizar el preprocesamiento, sustituiremos dichos valores vacíos por NA.
```{r}
mushroom[mushroom == ""] <- NA
```

Comprobamos que se han sustituido correctamente los valores vacíos por NA, pudiendo comprobar la cantidad de valores nulos que hay en cada variable.
Esta información nos ayudará a decidir si será conveniente eliminar dichas variables o no.
```{r}
colSums(is.na(mushroom))
```
Podemos observar que existen 5 variables donde más del 50% de los valores son nulos. Estas son: stem.surface, veil.color, spore.print.color, stem.root, veil.type.

En este caso, decidimos eliminar aquellas variables que cuentan con más del 50% de sus valores faltantes. La razón de esta decisión es que, si decidimos imputar los valores faltantes, estaríamos inventando demasiados datos. Imputar valores faltantes significa reemplazar los valores faltantes con algún valor que estimemos adecuado. Sin embargo, si una variable tiene más del 50% de sus valores faltantes, significa que estaríamos reemplazando más de la mitad de los valores de esa variable. Esto nos llevaría a tener un conjunto de datos con demasiados valores inventados, lo que podría afectar la precisión de nuestro análisis.
Para ello, en primer lugar, obtendremos el nombre de las columnas que queremos eliminar.

```{r}
nacols <- colnames(mushroom)[colSums(is.na(mushroom)) > nrow(mushroom) / 2]
print(nacols)
```

A continuación, eliminaremos dichas columnas del dataset.
```{r}
mushroom <- mushroom[, !names(mushroom) %in% nacols]
```

Comprobamos que se han eliminado correctamente las columnas.
```{r}
print(colnames(mushroom))
```

Para poder analizar de forma específica cada variable, separamos las variables numéricas de las categóricas.
```{r}
colsnames <- colnames(mushroom)
numerical_features <- c("cap.diameter", "stem.height", "stem.width")
categorical_features <- colsnames[!colsnames %in% numerical_features]
print(categorical_features)
print(numerical_features)
```

Comenzamos analizando las variables categóricas. Para ello visualizamos la distribución de las variables categóricas a través de histogramas. Observaremos los posibles valores de cada variable categórica junto con su frecuencia de aparición en el dataset.
```{r}
for (i in categorical_features) {
  print(ggplot(mushroom, aes_string(x = i)) +
    geom_bar(fill = "#7fd6d9") +
    geom_text(stat = "count", aes(label = scales::percent(..count.. / nrow(mushroom)), vjust = -0.25)) +
    labs(x = i, y = "Percentage") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)))
}
```

Tras visualizar los histogramas de cada variable, obtenemos las siguientes conclusiones:

* La variable dependiente "class" está balanceada, es decir, la frecuencia de aparición de "e" (edible) y "p" (poisonous) es similar, en una proporción de 44,5% y 55,5% respectivamente. Por lo tanto, no es necesario realizar un balanceo de la variable dependiente, permitiendo aplicar la medida de evaluación "accuracy" para evaluar el modelo.
* Siguen existiendo valores NA en algunas variables, pero estos serán imputados posteriormente con la moda de cada variable.
* La variable ring-type tiene 8 posibles valores, pero en el 80% de los casos, el valor es "f", por lo que quizás se podría eliminar esta variable en un futuro, ya que parece que aporta información irrelevante. Más adelante la analizaremos en profundidad.

A continuación, seguiremos con el análisis de las variables numéricas, donde visualizaremos su distribución respecto a la clase a través de una gráfica generada con "featurePlot". Esta requiere que la variable objetivo sea de tipo factor, por lo que hacemos la conversión.
```{r}
mushroom$class <- as.factor(mushroom$class)
featurePlot(x = mushroom[, numerical_features], y = mushroom$class, plot = "strip")
```
Con la gráfica anterior, se puede observar que para valores altos de cap.diameter, stem.height y stem.width, la probabilidad de que la clase sea "e" es mayor que la de "p". Por lo tanto, podemos deducir que si una seta es de tamaño grande, su probabilidad de ser comestible es bastante mayor.


Anteriormente hemos comentado que la variable "ring.type" tiene 8 posibles valores, pero en casi un 80% de los casos, el valor es "f". Para evaluar su posible eliminación o la de alguna otra variable, se puede utilizar la función "nearZeroVar". Esta función devuelve un vector con los índices de las variables que tienen una varianza cercana a 0.
```{r}
near_zero_col <- nearZeroVar(mushroom, saveMetrics = FALSE)
colnames(mushroom)[c(near_zero_col)]
```

Como suponíamos, la variable "ring.type" es la que tiene una varianza cercana a 0, y por tanto podría ser eliminada. A continuación, visualizaremos la correlación existente con la variable dependiente "class" para tomar una decisión.
```{r}
print(ggplot(mushroom, aes_string(x = "ring.type")) +
  geom_bar(aes(fill = class)))
```

Tras visualizar la gráfica anterior, se puede observar que la distribución de la variable dependiente "class" es similar en casi todos los valores de "ring.type". Sin embargo, en el caso de "ring.type" = "z" y "ring.type" = "m", la distribución de la variable dependiente "class" es diferente. Por lo tanto, se decide finalemnte mantener la variable "ring.type".

## Imputación de valores nulos<a name="imputation"></a>

Como se ha comentado anteriormente, los valores nulos de las variables categóricas se imputarán a través de la moda de cada variable. 
Este proceso lo hacemos de forma manual, ya que la función preProcess() de la librería caret no permite imputar valores nulos de variables categóricas.
```{r}
for (i in categorical_features) {
  mushroom[, i][is.na(mushroom[, i])] <- names(which.max(table(mushroom[, i])))
}
```

Comprobamos que ya no existen valores nulos en las variables categóricas.
```{r}
colSums(is.na(mushroom))
```

## Escalado de variables numéricas<a name="scaling"></a>

Con el objetivo de que todas las variables tengan la misma escala y evitar que una variable tenga más peso que otra, se escalarán las variables numéricas.
Para realizar este escalado, se utilizará la función preProcess() de la librería caret. Esta función devuelve un objeto de tipo "preProcess" que contiene la información necesaria para escalar las variables numéricas. 
A continuación, se realizará el escalado y se sustituirán las variables numéricas originales por las escaladas.
```{r}
range_numeric <- preProcess(mushroom[, numerical_features], method = c("range"))
mushroom[, numerical_features] <- predict(range_numeric, newdata = mushroom[, numerical_features])
str(mushroom)
```


# Análisis de datos<a name="analysis"></a>


## Análisis aprendizaje supervisado<a name="supervised"></a>

Para trabajar con el dataset mediante el aprendizaje supervisado; es decir, utilizando datos que son etiquetados mediante la intervención de un ser humano, utilizaremos diferentes tipos de clasificadores, algunos de ellos ya trabajados durante las clases prácticas de la asignatura y otros que eran desconocidos para nosotros y sobre los cuales hemos tenido que investigar anteriormente sobre su funcionamiento en R.
Los algoritmos de clasificación que hemos seleccionado y los cuales vamos a aplicar son:

  1. Regresión logística.
  
  2. KNN o el Vecino más cercano.
  
  3. Árboles de decisión.
  
  4. Random Forest.
  
  5. MSV o Máquina de Soporte Vectorial.
  
Una vez aplicados cada uno de los clasificadores, los compararemos entre sí y seleccionaremos el o los algoritmos que mayor precisión proporcionen sin llegar al sobreajuste, tratando de buscar que el resultado final sea generalizado para los datos.


En primer lugar, ante de comenzar a aplicar los clasificadores, dividiremos el dataset en dos conjuntos: el primer conjunto será el de entrenamiento o training y el segundo será el conjunto de prueba o test. El conjunto de entrenamiento lo utilizaremos para entrenar los distintos modelos y el conjunto de prueba lo utilizaremos para evaluar cada uno de ellos una vez obtenidos.

```{r}
library(caTools)
set.seed(18)

split <- sample.split(mushroom$class, SplitRatio = 0.8)
training_set <- subset(mushroom, split == TRUE)
test_set <- subset(mushroom, split == FALSE)

table(training_set$class)
table(test_set$class)
```


### Regresión Logística<a name="logistic"></a>

La regresión logística permite predecir el resultado de una variable categórica en función de las variables independientes o predictoras.
A continuación se muestra un resumen del conjunto de datos de entrenamiento con el cual vamos a trabajar una vez aplicada la función glm().

```{r}
rl_classiffier <- glm(class ~ ., family = binomial, data = training_set)
summary(rl_classiffier)
```

La función aplicada, glm(), obtiene los valores residuales del modelo y los coeficientes de ajuste para cada una de las variables independientes. Además, se obtiene el p-value correspondiente para cada una de ellas.
En el resumen mostrado mediante la función summary(), se pueden observar variables con dos o tres asteriscos, las cuales aportan bastante relevancia al modelo como predictores; sin embargo, las variables que poseen un solo asterisco o incluso ninguno, significa que apenas aportan relevancia a los resultados.
A continuación, procedemos a predecir las clases del conjunto de entrenamiento y de validación. Tomando como umbral '0,5', dividiremos los champiñones en comestibles y venenosos, de forma que si la probabilidad queda por encima de dicho umbral significará que el champiñón es comestible, y de lo contrario, si el resultado se mantiene por debajo, significará que el champiñón es venenoso. Para acabar, formaremos la matriz de confusión con los resultados de las predicciones para proceder a su análisis y valoración.

```{r}
pred_train <- predict(rl_classiffier, newdata = training_set, type = "response")
pred_train <- ifelse(pred_train > 0.5, "p", "e")
pred_train <- factor(pred_train, levels = c("e", "p"), labels = c("e", "p"))

confusion_m <- table(training_set$class, pred_train)
print(confusion_m)

accuracy <- sum(diag(confusion_m)) / sum(confusion_m)
print(accuracy)
```


Una vez obtenido el resultado mostrado mediante la matriz de confusión, se observa que la predicción posee una precisión del 77,8%.
A continuación, se muestra el resultado de aplicar el mismo proceso anterior sobre el conjunto de prueba.

```{r}
pred_test <- predict(rl_classiffier, newdata = test_set, type = "response")
pred_test <- ifelse(pred_test > 0.5, "p", "e")
pred_test <- factor(pred_test, levels = c("e", "p"), labels = c("e", "p"))

confusion_m <- table(test_set$class, pred_test)
print(confusion_m)

accuracy_rl <- sum(diag(confusion_m)) / sum(confusion_m)
print(accuracy_rl)
```

Una vez obtenido este segundo resultado, podemos observar que hay una gran similitud entre los resultados obtenidos para ambos conjuntos de datos, resultando esta vez en un 77,3%.
Antes de acabar de aplicar el clasificador de regresión logística, vamos a proceder a graficar la curva ROC, la cual nos aporta mayor visualización de la relación entre los falsos y verdaderos positivos.

```{r}
library(ROCR)
pred_rl_roc <- prediction(as.numeric(pred_test), as.numeric(test_set$class))
perf_rl_roc <- performance(pred_rl_roc, "tpr", "fpr")
perf_rl_auc <- performance(pred_rl_roc, "auc")

print(perf_rl_auc@y.values[[1]])
plot(perf_rl_roc, col = "lightblue", lwd = 5)  
```

Observando la curva ROC resultante podemos comentar que se mantiene por encima de la diagonal, lo que es buena señal, pero se aproxima a ella, pudiendo haber proporcionado resultados mejores resulta ser un modelo bastante generalizado.


### k-NN<a name="knn"></a>

Mediante el algoritmo de clasificación llamado k-NN se asigna una nueva observación a la clase más común entre sus "k" vecinos más cercanos en el espacio de características.
Antes de nada, para poder aplicar el algoritmo k-NN, debemos transformar las variables categóricas en numéricas.

```{r}
mushroom_num <- dummyVars(" ~ .", data = mushroom, fullRank = TRUE) %>% predict(mushroom)
mushroom_num <- as.data.frame(mushroom_num)
```

A continuación, se visualizan las variables categóricas ya codificadas y las dimensiones del dataset.

```{r}
dim_mushroom <- dim(mushroom_num)
print(dim_mushroom)
str(mushroom_num)
```

Debido al hecho de necesitar la transformación de las variables categóricas en numéricas, procedemos en este paso a dividir los datos en los conjuntos de entrenamiento y de validación. En este caso, el valor 0 corresponde con los champiñones comestibles y el valor 1 con los champiñones venenosos.

```{r}
library(caTools)
set.seed(18)

split <- sample.split(mushroom_num$class, SplitRatio = 0.8)
training_set_num <- subset(mushroom_num, split == TRUE)
test_set_num <- subset(mushroom_num, split == FALSE)

table(training_set_num$class)
table(test_set_num$class)
```

El siguiente paso será determinar el valor óptimo de k antes de proceder a aplicar la función. Para ello, utilizaremos el resultado que nos proporciona el cálculo de la raíz cuadrada del número de observaciones del conjunto de entrenamiento.

```{r}

nrows_class <- NROW(training_set_num) 
k <- sqrt(nrows_class)
k <- round(k)
k
```

Una vez hemos obtenido el valor de k, procedemos a realizar las predicciones. Para llevar a cabo la aplicación del clasificador llamado 'el vecino más cercano', haremos uso de la función knn() de la librería class.

```{r}
library(class)
set.seed(18)
pred_knn <- knn(train = training_set_num[, -1], test = test_set_num[, -1], cl = training_set_num$class, k = k)
summary(pred_knn)
```

Una vez obtenidos los resultados de las predicciones en este caso, podemos de igual forma construir la matriz de confusión.

```{r}
confusion_m <- table(test_set_num$class, pred_knn)
confusion_m

accuracy_knn <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_knn
```

Como resultado final de la matriz de confusión, obtenemos una precisión resultante del 97%, la cual mejora respecto al algoritmo de clasificación anterior, aunque tendiendo a poseer mayor sobreajuste.
De forma más visual, obtenemos a continuación la gráfica de la curva ROC y el cálculo del área bajo la misma.

```{r}
library(ROCR)
pred_knn_roc <- prediction(as.numeric(pred_knn), as.numeric(test_set_num$class))
perf_knn_roc <- performance(pred_knn_roc, "tpr", "fpr")
perf_knn_auc <- performance(pred_knn_roc, "auc")

print(perf_knn_auc@y.values[[1]])
plot(perf_knn_roc, col = "lightblue", lwd = 5)  
```

Una vez obtenemos la curva ROC y su área, observamos que el resultado es muy positivo en cuanto a precisión de los resultados, ya que como se puede observar gráficamente se aleja de la diagonal.


### Clasificación con Árbol de Decisión<a name="arbol"></a>

Los árboles de decisión se basan en la construcción de reglas lógicas (divisiones de los datos entre rangos o condiciones) a partir de los datos de entrada.
Para trabajar con este clasificador comenzamos aplicando la función del árbol de decisión, rpart(), sobre el conjunto de datos de entrenamiento como se muestra a continuación.

```{r}
library(rpart)
set.seed(18)
dt_classiffier <- rpart(class ~ ., data = training_set)
```
Una vez obtenido el resultado, lo graficaremos para facilitar así el análisis del resultado.

```{r}
library(rpart.plot)
rpart.plot(dt_classiffier)
```

Construimos la matriz de confusión con los resultados obtenidos anteriormente para este caso.

```{r}
pred_dt <- predict(dt_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_dt)
confusion_m

accuracy_dt <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_dt
```
Una vez tenemos el resultado para el valor de precisión en la predicción aplicando el árbol de decisión, 83%, procedemos a construir la gráfica de la curva ROC y a calcular el valor correspondiente al AUC. En este caso, continúa siendo mejor resultado que el obtenido mediante la regresión logística, puesto que resulta en una mayor precisión, pero al igual que el algoritmo de k-NN, tiende a ser más sobreajustado.

```{r}
library(ROCR)
pred_dt_roc <- prediction(as.numeric(pred_dt), as.numeric(test_set$class))
perf_dt_roc <- performance(pred_dt_roc, "tpr", "fpr")
perf_dt_auc <- performance(pred_dt_roc, "auc")

print(perf_dt_auc@y.values[[1]])
plot(perf_dt_roc, col = "lightblue", lwd = 5)  
```


### Clasificador Random Forest<a name="random"></a>

El algoritmo de Random Forest trabaja mediante la combinación de árboles predictores tal que cada árbol depende de los valores de un vector aleatorio.
Comenzamos aplicando dicho clasificador mediante la función llamada de la misma forma; es decir, randomForest(). En esta función, el valor correspondiente al parámetro llamado 'ntree' indica la cantidad de árboles de decisión que formarán parte del clasificador.

```{r}
library(randomForest)
set.seed(18)
rf_classiffier <- randomForest(class ~ ., data = training_set, ntree = 250)
```

Mediante su gráfica, vamos a proceder a comparar los errores en función del aumento del número de árboles; es decir, cuanto más vaya aumentando el número de árboles hasta un umbral determinado, menor cantidad de errores poseerá la predicción.

```{r}
plot(rf_classiffier)
```

Calculamos las predicciones sobre el conjunto de datos de prueba y construimos la matriz de confusión.

```{r}
pred_rf <- predict(rf_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_rf)
confusion_m

accuracy_rf <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_rf
```

Una vez obtenido el valor de la precisión para este caso, definimos la curva ROC y procedemos a realizar el cálculo del área bajo la curva. Mediante este último paso se puede observar su tan alta precisión, la cual indica demasiado sobreajuste sin ser conveniente.

```{r}
library(ROCR)
pred_rf_roc <- prediction(as.numeric(pred_rf), as.numeric(test_set$class))
perf_rf_roc <- performance(pred_rf_roc, "tpr", "fpr")
perf_rf_auc <- performance(pred_rf_roc, "auc")

print(perf_rf_auc@y.values[[1]])
plot(perf_rf_roc, col = "lightblue", lwd = 5)  
```


### Kernel SVM Classifier

El clasificador de la máquina vectorial, encuentra la curva que es capaz de separar y clasificar los datos de entrenamiento garantizando que la separación entre ésta y ciertas observaciones del conjunto de entrenamiento resulte ser lo mayor posible.
Para llevar a cabo la aplicación del clasificador llamado 'Máquina de Soporte Vectorial' hacemos uso de la función svm(). En dicha función, los valores de los parámetros 'type' y 'kernel' hacen referencia al tipo de clasificador lo que significa que el kernel será de tipo radial y gaussiano. 

```{r}
library(e1071)
set.seed(18)
svm_classiffier <- svm(class ~ .,
  data = training_set,
  type = "C-classification", kernel = "radial"
)
```
A continuación, se calcula la predicción y se construye la matríz de confusión, las cuales resultan ser las siguientes.

```{r}
pred_svm <- predict(svm_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_svm)
confusion_m

accuracy_svm <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_svm
```

Como se puede observar, el valor de la precisión en la predicción en este caso resulta ser del 95%, un resultado bueno que no muestra señales de sobreajuste.
Para finalizar, construimos la curva ROC correspondiente en este caso y calculamos su área.

```{r}
library(ROCR)
pred_svm_roc <- prediction(as.numeric(pred_svm), as.numeric(test_set$class))
perf_svm_roc <- performance(pred_svm_roc, "tpr", "fpr")
perf_svm_auc <- performance(pred_svm_roc, "auc")

print(perf_svm_auc@y.values[[1]])
plot(perf_svm_roc, col = "lightblue", lwd = 5)  
```


### Conclusiones

Una vez aplicados los cinco distintos métodos de clasificación sobre nuestro dataset llamado 'mushroom' tras haber realizado antes su preprocesamiento, podemos concluir diciendo que el clasificador de Random Forest es el que ha resultado poseer un mayor valor en la precisión de la predicción en la clasificación y, por lo tanto, un menor valor para el error de predicción. Sin embargo, al aplicar este algoritmo hemos obtenido un mayor sobreajuste, el cual no beneficia al modelo ya que se busca que los resultados obtenidos sean precisos, pero también generalizados para los datos. Por esta razón, consideramos que resulta más beneficioso sacrificar parte del valor de precisión, teniendo en cuenta algunos valores de falsos positivos y falsos negativos, como ocurre en el caso de los clasificadores de la Máquina de Soporte Vectorial o k-NN, aprovechando así su capacidad de mayor generalización.
Sobre los gráficos que se muestran a continuación se pueden comparar los distintos niveles de precisión y AUC para cada uno de los diferentes clasificadores que han sido aplicados.

```{r}
accuracy_comp <- matrix(c(accuracy_rl, accuracy_knn, accuracy_dt, accuracy_rf, accuracy_svm), ncol = 5)

barplot(accuracy_comp,
  main = "Accuracy Comparison",
  xlab = "Accuracy (%)",
  ylab = "Method",
  names.arg = c("RL", "K-NN", "DT", "RF", "SVM"),
  col = "#7fd6d9"
)
```

```{r}
perf_auc <- matrix(c(perf_rl_auc@y.values[[1]], perf_knn_auc@y.values[[1]], perf_dt_auc@y.values[[1]], perf_rf_auc@y.values[[1]], perf_svm_auc@y.values[[1]]), ncol = 5)

barplot(perf_auc,
  main = "AUC Comparison",
  xlab = "AUC (%)",
  ylab = "Method",
  names.arg = c("RL", "K-NN", "DT", "RF", "SVM"),
  col = "#7fd6d9"
)
```


## Análisis aprendizaje no supervisado<a name="unsupervised"></a>

En este apartado se analizará el dataset a través de algoritmos de aprendizaje no supervisado. En concreto, se probarán los algoritmos k-means y clustering jerárquico.
Para ambos algoritmos, se seguirá el siguiente esquema:

Para ambos algoritmos se llevará a cabo el siguiente proceso:

1. Se representará gráficamente la distribución inicial de los datos en el dataset.
2. Se determinará el número óptimo de clústeres a utilizar para dividir los datos de forma adecuada.
3. Se representará gráficamente la distribución de los datos en función del número de clústeres elegido.
4. Se calculará el promedio de cada una de las variables en el dataset para cada uno de los clústeres resultantes.
5. Finalmente, y gracias a tener información a priori de la clase de cada champiñón, se calculará el "accuracy" del algoritmo, midiendo cómo de bien se está realizando la tarea de dividir los datos en clústeres de forma adecuada.

Para poder trabajar con algoritmos no supervisados será necesario que las variables sean numéricas. Para ello, se eliminarán las variables categóricas del dataset.
```{r}
numerical_columns <- mushroom[, numerical_features]
```

Antes de comenzar con los algoritmos no supervisados, representaremos de forma gráfica la distribución inicial de los datos a través de un diagrama de dispersión 3D, dónde cada punto representa un champiñón y las variables que se representan son el diámetro del sombrero, la altura del tallo y el ancho del tallo. Cada variable está normalizada entre 0 y 1.
```{r}
df <- as.data.frame(numerical_columns)

plot_ly(df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width
) %>%
  add_markers(size = 1.5)
```

### K-means<a name="kmeans"></a>

El algoritmo k-means es un método de clustering que permite dividir un conjunto de datos en k grupos o clústeres de manera que los puntos dentro de un mismo clústeres sean similares entre sí y diferentes a los puntos de los demás clústeres. La función kmeans() de la librería clústeres una implementación del algoritmo k-means en R.

Para utilizar la función kmeans(), es necesario especificar el número de clústeres que se desean obtener, que se indica a través del parámetro "centers". 

Por otro lado, el parámetro "nstar" indica el número de veces que se desea realizar el proceso de clustering. Cada vez que se ejecuta el proceso, se utiliza un conjunto diferente de semillas iniciales para los centroides de los clústeres y se obtiene un resultado diferente. Al especificar un valor para "nstart" mayor que 1, se obtienen varios resultados diferentes y se selecciona el que minimiza la suma de cuadrados total. Por tanto, establecemos "nstart" a 20 para que se realice el proceso 20 veces y se obtenga un resultado más robusto.

Una vez que se ha ejecutado la función con un determinado valor de "centers", se puede calcular la suma de cuadrados internos (within groups sum of squares) para ese valor de "centers". La suma de cuadrados internos es una medida de la variabilidad de los datos dentro de cada cluster. Cuanto mayor sea la suma de cuadrados internos, más dispersos estarán los datos dentro del clústery, por tanto, menos homogéneo será el cluster.

Para determinar el número óptimo de clústeres, se puede utilizar el método del codo, que consiste en representar la suma de cuadrados internos en función del número de clústeres y seleccionar el número de clústeres en el que se produce un "codo" en la gráfica. Este "codo" suele corresponder al punto en el que la disminución de la suma de cuadrados internos se vuelve más lenta y, por tanto, a partir del cual no se obtienen mejoras significativas en la calidad del clustering.

```{r}
wss_per_k <- 0
for (i in 1:10) {
  kmeans_aux <- kmeans(numerical_columns, center = i, nstar = 20)
  wss_per_k[i] <- kmeans_aux$tot.withinss
}
par(mfrow = c(1, 1))
plot(1:10, wss_per_k,
  type = "b",
  xlab = "Number of clusters",
  ylab = "WSS",
)
```

Como se puede observar en la gráfica anterior, la suma de cuadrados internos disminuye a medida que aumenta el número de clústeres. Sin embargo, a partir de 2 clústeres, la disminución de la suma de cuadrados internos es más pequeña. Por lo tanto, se decide hacer uso 2 clústeres. En este caso específico, tiene sentido utilizar 2 clústeres, ya que conocemos que el dataset es binario.

Una vez determinado el número de clústeres, generamos el modelo de k-means.

```{r}
km_model <- kmeans(df, center = 2, nstar = 20)
```

Para poder visualizar los resultados, se añade una nueva columna al dataset con el número de clúster al que pertenece cada observación. Una vez añadida, se puede representar la distribución de los datos en función de los clústeres obtenidos.
```{r}
df$cluster<- factor(km_model$cluster)

plot_ly(df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width, color = ~cluster
) %>%
  add_markers(size = 1.5)
```

Se puede observar que los champiñones de menor tamaño (en diámetro, altura y anchura) pertenecen al clúster 2 y los de mayor tamaño pertenecen al clúster 1.

A continuación, calcularemos el valor promedio de las variables para cada clúster generado con el modelo de k-means. Para ello, utilizaremos la función group_by() de la librería dplyr para agrupar los datos por clúster y la función summarise() para calcular el valor promedio de cada variable. Es importante destacar que, en este caso, los datos están normalizados, por lo que el valor promedio de cada variable no tiene un significado real. Sin embargo, nos permite comparar los valores de cada variable para cada cluster.
```{r}
grouped_mushroom <- df %>%
  group_by(cluster) %>%
  summarise(
    mean_cap_diameter = mean(cap.diameter),
    mean_stem_height = mean(stem.height),
    mean_stem_width = mean(stem.width)
  )

grouped_mushroom
```

Observamos que los valores del clúster 1 son mayores que los del clúster 2, lo que indica que los champiñones del clúster 1 son de mayor tamaño que los del clúster 2. 

A partir de este momento, hemos decidido modificar el dataset actual debido a que para aplicar técnicas como "silhouette" o "dendrogram" (para el caso de clustering jerárquico) es necesario que el dataset sea de menor tamaño.
La función createDataPartition() de la librería caret en R permite dividir un conjunto de datos en dos grupos, uno de entrenamiento y otro de validación, de manera estratificada, es decir, manteniendo la proporción de elementos de cada clase en ambos grupos. En este caso, se está especificando que se desea mantener únicamente el 1% de los datos iniciales para el análisis, lo que implica que se está utilizando createDataPartition() para reducir el tamaño del conjunto de datos en lugar de para dividirlo en dos grupos. Al utilizar createDataPartition() de esta manera, se mantiene la proporción de elementos de cada clase en el conjunto de datos reducido.

Antes de reducir el dataset, es necesario convertir las variables categóricas en numéricas a través de variables dummy. Para ello, utilizamos la función dummyVars() de la librería caret para crear un objeto de tipo dummyVars y la función predict() para crear un nuevo dataset con las variables dummy.

```{r}
mushroom <- dummyVars(" ~ .", data = mushroom, fullRank = TRUE) %>% predict(mushroom)
mushroom <- as.data.frame(mushroom)

set.seed(42)
split <- createDataPartition(mushroom$class, p = 0.01)
smaller_df <- mushroom[split$Resample1, ]
```

Comprobamos que la proporción de datos de cada clase se mantiene al hacer la partición.
```{r}
initial_class_prop <- table(mushroom$class) / nrow(mushroom)
smaller_class_prop <- table(smaller_df$class) / nrow(smaller_df)

print(initial_class_prop)
print(smaller_class_prop)
```

Mostramos de forma gráfica las nuevas proporciones de la variable dependiente "class" en el dataset reducido.
```{r}
print(ggplot(smaller_df, aes_string(x = smaller_df$class)) +
  geom_bar(fill = "#7fd6d9") +
  geom_text(stat = "count", aes(label = scales::percent(..count.. / nrow(smaller_df)), vjust = -0.25)) +
  labs(x = i, y = "Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)))
```


Una vez reducido el dataset, vamos a eliminar las variables categóricas para poder aplicar técnicas de clustering.
```{r}
smaller_df <- smaller_df[, numerical_features]
dim(smaller_df)
```

Tras ejecutar la celda anterior se puede observar que el dataset ha pasado a tener 611 observaciones y 3 variables.

Como nos encontramos ante un dataset "nuevo", en primer lugar, visualizaremos la distribución inicial de los datos a través de una gráfica 3D.

```{r}
plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width
) %>%
  add_markers(size = 1.5)
```

A continuación, vamos a estudiar cuál sería el número óptimo de clústeres para el dataset reducido haciendo uso de la medida de bondad interna "silhouette". Para ello, utilizaremos la función fviz_nbclust de factoextra.

La medida silhouette toma valores entre -1 y 1. Un valor cercano a 1 indica que el punto está bien asignado al clúster y los puntos del clúster son muy similares entre sí. Un valor cercano a 0 indica que el punto está en un "área gris" y no está claramente asignado a ninguno de los dos clústeres. Un valor cercano a -1 indica que el punto está mal asignado al clúster y sería más apropiado para otro clúster.

La función fviz_nbclust() de la librería factoextra en R permite visualizar la medida silhouette para diferentes valores de "k" (número de clústeres) y ayudar a determinar el número óptimo de clústeres. Al utilizar esta función, se puede obtener un gráfico en el que se representa la medida silhouette en función del número de clústeres y seleccionar el valor de "k" en el que se obtiene el mayor valor de silhouette.
```{r}
fviz_nbclust(smaller_df, FUNcluster = kmeans, method = "silhouette")
```

Según la gráfica, podemos afirmar que el número óptimo de clústeres es 2, ya que es el valor de "k" que maximiza la medida silhouette. También podemos observar que el valor de silhouette para "k" = 3 es cercano al obtenido para "k" = 2. Esto puede ser debido a que nuestro dataset, pese a ser binario, cuenta con datos muy dispersos.

Por último, visualizamos la distribución de los datos en función de los clústeres obtenidos.

```{r}
km_sm_model <- kmeans(smaller_df, center = 2, nstart = 20)
cluster <- factor(km_sm_model$cluster)

plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width, color = ~cluster
) %>%
  add_markers(size = 1.5)
```

Por último, calculamos el valor promedio de las variables para cada clúster generado con el modelo de k-means para el dataset reducido.
```{r}
grouped_sm_mushroom <- smaller_df %>%
  mutate(cluster = cluster) %>%
  group_by(cluster) %>%
  summarise(
    mean_cap_diameter = mean(cap.diameter),
    mean_stem_height = mean(stem.height),
    mean_stem_width = mean(stem.width)
  )
grouped_sm_mushroom
```

### Clustering jerárquico<a name="hierarchical"></a>

Clustering jerárquico es un tipo de algoritmo de clustering que se utiliza para dividir a un conjunto de datos en grupos (clústeres) de forma que los datos en el mismo clústersean similares entre sí. La función hclust es una función en R que se utiliza para realizar clustering jerárquico.
 
Antes de aplicar el algoritmo hclust, es necesario calcular las distancias entre los puntos del conjunto de datos, para ello, utilizaremos la función dist de R. La función dist() calcula la distancia entre los puntos del conjunto de datos y devuelve una matriz de distancias. Por defecto, la función dist() utiliza la distancia euclídea para calcular las distancias entre los puntos del conjunto de datos.

Cabe destacar, que la función hclust hace uso por defecto del cáculo de distancia entre clústeres basado en el método de "Complete". Este método de cálculo de distancia entre clústeres se basa en la distancia entre los puntos más lejanos de cada cluster.
```{r}
distance <- dist(smaller_df)
hc_model <- hclust(distance)
```

Representamos el dendrograma para visualizar la distribución de los datos en función de los clústeres obtenidos.
```{r}
dend_modelo <- as.dendrogram(hc_model)
plot(dend_modelo, ylab = "Similarity")
```

Hasta ahora, hemos obtenido la jerarquía de los datos, pero lo que realmente nos interesa es la clasificación de los datos en función de los clústeres.
Cortaremos el dendrograma en un punto que nos interese para obtener los clústeres. En este caso, y a modo de prueba, hemos decidido cortar el dendrograma en 90 para obtener una visualización del dendograma cortado.
```{r}
cut <- 0.9

dend_modelo %>%
  color_branches(h = cut) %>%
  color_labels(h = cut) %>%
  plot(ylab = "Similarity")
```

Para obtener el número óptimo de clúster, haremos uso de la medida interna de bondad silhouette. Para ello, utilizaremos la función fviz_nbclust de factoextra al igual que con k-means.

```{r}
fviz_nbclust(smaller_df, FUNcluster = hcut, method = "silhouette")
```

Comprobamos que en este caso, el número óptimo de clústeres podría ser 2 o 3, ya que el valor de silhouette es muy similar para ambos casos. En este caso, hemos decidido utilizar 2 clústeres para poder comparar posteriormente los resultados con los obtenidos con el algoritmo de k-means.

Para generar el modelo de clustering jerárquico, utilizaremos la función cutree de R. Esta función nos permite generar el modelo de clustering jerárquico en función del número de clústeres que queramos obtener.

Calculamos la agrupación del modelo en función del número de clústeres que hemos decidido utilizar, y calculamos el valor promedio de las variables para cada clústergenerado.

```{r}
jq_cluster <- cutree(hc_model, k = 2)

grouped_mushroom <- smaller_df %>%
  mutate(cluster = jq_cluster) %>%
  group_by(cluster) %>%
  summarise_all(mean)
grouped_mushroom
```

Visualizamos la agrupación de los datos en función de los clústeres obtenidos a partir del modelo de clustering jerárquico.

```{r}
jq_cluster<- factor(jq_cluster)

plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width,
  color = ~jq_cluster
) %>%
  add_markers(size = 1.5)
```

Con el objetivo de comparar los resultados obtenidos en los dos algoritmos, vamos a calcular el rendimiento de cada uno de ellos, haciendo uso del accuracy como medida de bondad externa.

En primer lugar, calculamos el accuracy del modelo de k-means. Supondremos que la clase 1 es la clase "e" y la clase 2 es la clase "p".
Para ello, obtenemos las clases reales y las clases predichas, y calculamos el accuracy.

Primero necesitamos volver a obtener el dataset reducido para poder tener las clases reales.
```{r}
smaller_df <- mushroom[split$Resample1, ]
```

```{r}
real_classes <- ifelse(smaller_df$class == "e", 1, 2)
predicted_classes <- km_sm_model$cluster
predicted_classes <- as.numeric(predicted_classes)
```

```{r}
accuracy <- sum(real_classes == predicted_classes) / length(real_classes)
print(accuracy)
```

Hacemos lo mismo con el modelo de clustering jerárquico, pero en este caso, supondremos que la clase 1 es la clase "p" y la clase 2 es la clase "e".
```{r}
real_classes <- ifelse(smaller_df$class == "e", 2, 1)
predicted_classes <- as.numeric(jq_cluster)
```

```{r}
accuracy <- sum(real_classes == predicted_classes) / length(real_classes)
print(accuracy)
```

Tras comparar los resultados obtenidos en los dos algoritmos, podemos afirmar que el modelo de clustering jerárquico ha obtenido un accuracy mayor para este dataset, obteniendo un accuracy del 98% frente al 70% obtenido por el modelo de k-means.

Pese a obtener un accuracy mayor con el modelo de clustering jerárquico, hay que tener en cuenta que el objetivo de aprendizaje no supervisado es la agrupación de los datos en función de sus características, y no la predicción de una variable objetivo. Además, el accuracy obtenido con el modelo de clustering jerárquico es muy alto, lo que puede deberse a que el dataset utilizado es muy reducido y no presenta mucha variabilidad entre las clases.
