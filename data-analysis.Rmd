---
title: "Mushroom Data Analysis"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---

# Índice   
1. [Introducción](#introduction) \
2. [Cargar librerías](#libraries) \
3. [Cargar datos](#data) \
4. [Visualización y preprocesamiento](#preprocessing) \
4.1 [Imputación de valores nulos](#imputation) \
4.2 [Codificación de variables categóricas](#encoding) \
4.3 [Escalado de variables numéricas](#scaling)
5. [Análisis de datos](#analysis) \
5.1 [Análisis aprendizaje supervisado](#supervised) \
5.2 [Análisis aprendizaje no supervisado](#unsupervised) \
5.2.1 [K-means](#kmeans) \
5.2.2 [Clustering jerárquico](#hierarchical) \

\pagebreak

# Introducción<a name="introduction"></a>

Este trabajo se centra en el análisis de un conjunto de datos que contiene información sobre 61069 champiñones diferentes. Los datos han sido obtenidos de Kaggle y cada una de las instancias incluye 21 variables que describen diferentes aspectos de los champiñones, como su forma, tamaño y hábitat. En estas variables se encuentra inlcuida la clase de cada champiñón, indicando si es venenoso o comestible.
Antes de comenzar el análisis, es necesario realizar una fase de preprocesamiento de los datos. Esta fase tiene como objetivo preparar los datos para su análisis y obtener mayor conocimiento sobre estos. Durante esta fase, realizaremos tareas como visualizar los datos, detectar y tratar valores faltantes, o normalizar los datos.
Una vez preparados los datos, procederemos a realizar el análisis. Para ello, utilizaremos tanto técnicas de aprendizaje supervisado como no supervisado. El aprendizaje supervisado implica entrenar un modelo con datos etiquetados, es decir, que ya conocemos la clase de cada instancia. Una vez entrenado el modelo, podemos utilizarlo para predecir la clase de nuevas instancias. En este caso, utilizaremos diferentes algoritmos de clasificación para evaluar diferentes modelos.
El aprendizaje no supervisado, por otro lado, no requiere de datos etiquetados. En este caso, el objetivo es agrupar las instancias en diferentes grupos de forma que las instancias de un mismo grupo sean similares entre sí y diferentes a las de los demás grupos. Utilizaremos dos algoritmos de clustering para evaluar diferentes modelos. Además, cabe destacar, que en nuestro caso contamos con un conocimiento a priori de los datos, ya que conocemos la clase de cada instancia. Por tanto, podemos utilizar este conocimiento para evaluar los modelos de clustering.

A continuación se detalla los posibles valores que pueden tomar las variables del dataset:
**1. class:** edible=e, poisonous=p \
**2. cap-diameter:** float number in cm \
**3. cap-shape:** bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s \
**4. cap-surface:** fibrous=f, grooves=g, scaly=y, smooth=s \
**5. cap-color:** brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y \
**6. does.bruise.or.bleed:** bruises-or-bleeding=t,no=f
**7. gill-attachment:** attached=a, descending=d, free=f, notched=n \
**8. gill-spacing:** close=c, crowded=w, distant=d \
**9. gill-color:** black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \
**10. stem-height:** float number in cm \
**11. stem-width:** float number in mm  \
**12. stem-root:** bulbous=b, swollen=s, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r \
**13. stem-surface:** see cap-surface + none=f \
**14. stem-color:** see cap-color + none=f \
**15. veil-type:** partial=p, universal=u \
**16. veil-color:** see cap-color + none=f \
**17. has-ring:** ring=t, none=f \
**18. ring-type:** cobwebby=c, evanescent=e, flaring=r, grooved=g, large=l, pendant=p, sheathing=s, zone=z, scaly=y, movable=m, none=f, unknown=? \
**19. spore-print-color:** see cap color \
**20. habitat:** grasses=g, leaves=l, meadows=m, paths=p, heaths=h, urban=u, waste=w, woods=d \
**21. season:** spring=s, summer=u, autumn=a, winter=w \

# Autores<a name="authors"></a>
Este trabajo ha sido realizado por: 
- Ana Diaz
- Deyan Rosenov
- Javier Vilariño
- María Isabel Ramos

Todos los ingrantes del grupo hemos realizado de forma conjunta la visualización y el preprocesamiento de los datos. Por otro lado, Ana y Deyan se ha encargado de realizar el aprendizaje supervisado, mientras que Javier y María Isabel se han encargado de realizar el aprendizaje no supervisado.


# Cargar librerías<a name="libraries"></a>

En los siguientes fragmentos se incluye el código necesario para instalar y cargar las librerías necesarias para el análisis de los datos realizado.
```{r}
install.packages("caret")
install.packages("tidyverse")
install.packages("plotly")
install.packages("dplyr")
install.packages("factoextra")
install.packages("dendextend")
```
```{r}
library(caret)
library(tidyverse)
library(plotly)
library(dplyr)
library(cluster)
library(factoextra)
library(dendextend)
```

# Cargar datos<a name="data"></a>

En primer lugar, realizamos la lectura del fichero csv, separando las columnas por ";" y mostramos las primeras 6 filas del fichero. Este código nos permitirá acceder a los datos de champiñones y trabajar con ellos en nuestro código R.
```{r}
mushroom <- read.csv("./data/data.csv", sep = ";")
head(mushroom)
```

Echamos un vistazo a las características de los atributos del dataset. En el caso de las variables numéricas, se puede observar valores como el mínimo, máximo, media, desviación estándar, etc. Por otro lado, en el caso de las variables categóricas no obtenemos información relevante.
```{r}
summary(mushroom)
```

# Visualización y preprocesamiento<a name="preprocessing"></a>

En primer lugar, vamos a comprobar si existen valores nulos en el dataset. Para ello, utilizaremos la función colSums(is.na(mushroom)), que nos devolverá la suma de valores nulos de cada variable.
```{r}
colSums(is.na(mushroom))
```

Según el resultado obtenido anteriormente, no existen valores nulos en el dataset. Sin embargo, si observamos el dataset, podemos observar que existen valores vacíos. Para poder trabajar con ellos, y que no nos de problemas a la hora de realizar el preprocesamiento, sustituiremos dichos valores vacíos por NA.
```{r}
mushroom[mushroom == ""] <- NA
```

Comprobamos que se han sustituido correctamente los valores vacíos por NA, y ahora si podemos comprobar la cantidad de valores nulos que hay en cada variable.
Esta información nos ayudará a decidir si será conveniente eliminar dichas variables o no.
```{r}
colSums(is.na(mushroom))
```
Podemos observar que existen 5 variables donde más del 50% de los valores son nulos. Estas son: stem.surface, veil.color, spore.print.color, stem.root, veil.type.

En este caso, decidimos eliminar aquellas variables que tenían más del 50% de sus valores faltantes. La razón de esta decisión es que, si decidimos imputar (es decir, reemplazar) los valores faltantes, estaríamos inventando demasiados datos. Imputar valores faltantes significa reemplazar los valores faltantes con algún valor que estimemos adecuado. Sin embargo, si una variable tiene más del 50% de sus valores faltantes, significa que estaríamos reemplazando más de la mitad de los valores de esa variable. Esto nos llevaría a tener un conjunto de datos con demasiados valores inventados, lo que podría afectar la precisión de nuestro análisis.
Para ello, en primer lugar, obtendremos el nombre de las columnas que queremos eliminar.

```{r}
nacols <- colnames(mushroom)[colSums(is.na(mushroom)) > nrow(mushroom) / 2]
print(nacols)
```

A continuación, eliminaremos dichas columnas del dataset.
```{r}
mushroom <- mushroom[, !names(mushroom) %in% nacols]
```

Comprobamos que se han eliminado correctamente las columnas.
```{r}
print(colnames(mushroom))
```

Para poder analizar de forma específica cada variable, separamos las variables numéricas de las categóricas.
```{r}
colsnames <- colnames(mushroom)
numerical_features <- c("cap.diameter", "stem.height", "stem.width")
categorical_features <- colsnames[!colsnames %in% numerical_features]
print(categorical_features)
print(numerical_features)
```

Comenzamos analizando las variables categóricas. Para ello visualizamos la distribución de las variables categóricas a través de histogramas. Observaremos los posibles valores de cada variable categórica junto con su frecuencia de aparición en el dataset.
```{r}
for (i in categorical_features) {
  print(ggplot(mushroom, aes_string(x = i)) +
    geom_bar(fill = "#7fd6d9") +
    geom_text(stat = "count", aes(label = scales::percent(..count.. / nrow(mushroom)), vjust = -0.25)) +
    labs(x = i, y = "Percentage") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)))
}
```

Tras visualizar los histogramas de cada variable, obtenemos a las siguientes conclusiones:

* La variable dependiente "class" está balanceada, es decir, la frecuencia de aparición de "e" (edible) y "p" (poisonous) es similar, en una proporción de 44,5% y 55,5% respectivamente. Por lo tanto, no es necesario realizar un balanceo de la variable dependiente, permitiendo aplicar la medida de evaluación "accuracy" para evaluar el modelo.
* Siguen existiendo valores NA en algunas variables, pero estos serán imputados posteriormente con la moda de cada variable.
* La variable ring-type tiene 8 posibles valores, pero en casi un 80% de los casos, el valor es "f", por lo que quizás se podría eliminar esta variable en un futuro, ya que parece que aporta información relevante. Más adelante la analizaremos en profundidad.

A continuación, seguiremos con el análisis de las variables numéricas, donde visualizaremos su distribución respecto a la clase a través de una gráfica generada con "featurePlot". Esta requiere que la variable objetivo sea de tipo factor, por lo que hacemos la conversión.
```{r}
mushroom$class <- as.factor(mushroom$class)
featurePlot(x = mushroom[, numerical_features], y = mushroom$class, plot = "strip")
```
Con la gráfica anterior, se puede observar que para valores altos de cap.diameter, stem.height y stem.width, la probabilidad de que la clase sea "e" es mayor que la de "p". Por lo tanto, podemos deducir que si una seta es de tamaño grande, su probabilidad de ser comestible es bastante mayor.


Anteriormente hemos comentado que la variable "ring.type" tiene 8 posibles valores, pero en casi un 80% de los casos, el valor es "f". Para evaluar su posible eliminación o la de alguna otra variable, se puede utilizar la función "nearZeroVar". Esta función devuelve un vector con los índices de las variables que tienen una varianza cercana a 0.
```{r}
near_zero_col <- nearZeroVar(mushroom, saveMetrics = FALSE)
colnames(mushroom)[c(near_zero_col)]
```

Como suponíamos, la variable "ring.type" es la que tiene una varianza cercana a 0, y por tanto podría ser eliminada. A continuación, visualizaremos la correlación existente con la variable dependiente "class" para tomar una decisión.
```{r}
print(ggplot(mushroom, aes_string(x = "ring.type")) +
  geom_bar(aes(fill = class)))
```

Tras visualizar la gráfica anterior, se puede observar que la distribución de la variable dependiente "class" es similar en casi todos los valores de "ring.type". Sin embargo, en el caso de "ring.type" = "z" y "ring.type" = "m", la distribución de la variable dependiente "class" es diferente. Por lo tanto, se decide finalemnte mantener la variable "ring.type".

## Imputación de valores nulos<a name="imputation"></a>

Como se ha comentado anteriormente, los valores nulos de las variables categóricas se imputarán a través de la moda de cada variable. 
Este proceso lo hacemos de forma manual, ya que la función preProcess() de la librería caret no permite imputar valores nulos de variables categóricas.
```{r}
for (i in categorical_features) {
  mushroom[, i][is.na(mushroom[, i])] <- names(which.max(table(mushroom[, i])))
}
```

Comprobamos que ya no existen valores nulos en las variables categóricas.
```{r}
colSums(is.na(mushroom))
```

## Escalado de variables numéricas<a name="scaling"></a>

Con el objetivo de que todas las variables tengan la misma escala para evitar que una variable tenga más peso que otra, se escalarán las variables numéricas.
Para realizar el escalado las variables numéricas, se utilizará la función preProcess() de la librería caret. Esta función devuelve un objeto de tipo "preProcess" que contiene la información necesaria para escalar las variables numéricas. 
A continuación, se realizara el escalado y se sustituiran las variables numéricas originales por las escaladas.
```{r}
range_numeric <- preProcess(mushroom[, numerical_features], method = c("range"))
mushroom[, numerical_features] <- predict(range_numeric, newdata = mushroom[, numerical_features])
str(mushroom)
```

# Análisis de datos<a name="analysis"></a>

## Análisis aprendizaje supervisado<a name="supervised"></a>

### Regresión Logística<a name="logistic"></a>

Primero se dividirá el dataset en dos conjuntos: uno de entrenamiento y otro de test. El conjunto de entrenamiento se utilizará para entrenar los modelos y el conjunto de test se utilizará para evaluarlos.

```{r}
library(caTools)
set.seed(18)

split <- sample.split(mushroom$class, SplitRatio = 0.8)
training_set <- subset(mushroom, split == TRUE)
test_set <- subset(mushroom, split == FALSE)

table(training_set$class)
table(test_set$class)
```

La regresión logística permite predecir el resultado de una variable categórica en función de las variables independientes o predictoras.

```{r}
rl_classiffier <- glm(class ~ ., family = binomial, data = training_set)
summary(rl_classiffier)
```
Una vez hemos aplicado la función glm(), obtenemos los valores residuales del modelo y los coeficientes de ajuste para cada una de las variables independientes.
Además, obtenemos el p-value correspondiente a cada una de ellas. Las variables con dos o tres asteriscos aportan bastante relevancia como predictores; sin embargo, las variables que no poseen ninguno o uno son de menor relevancia.

A continuación, procedemos a predecir las clases del conjunto de entrenamiento y de validación. Tomamos como umbral 0,5, de forma que si la probabilidad queda por encima de dicho umbral es que es comestible, pero si queda por debajo es que es venenoso.
Por último crearemos la matriz de confusión para valorar los resultados de las predicciones.

```{r}
pred_train <- predict(rl_classiffier, newdata = training_set, type = "response")
pred_train <- ifelse(pred_train > 0.5, "p", "e")
pred_train <- factor(pred_train, levels = c("e", "p"), labels = c("e", "p"))

confusion_m <- table(training_set$class, pred_train)
print(confusion_m)

accuracy <- sum(diag(confusion_m)) / sum(confusion_m)
print(accuracy)
```
Una vez hemos visto el resultado que muestra la matriz de confusión, se observa que la predicción tiene una precisión del 77,8%, lo que puede resultar algo baja.

Repetimos el mismo proceso para el conjunto de test.

```{r}
pred_test <- predict(rl_classiffier, newdata = test_set, type = "response")
pred_test <- ifelse(pred_test > 0.5, "p", "e")
pred_test <- factor(pred_test, levels = c("e", "p"), labels = c("e", "p"))

confusion_m <- table(test_set$class, pred_test)
print(confusion_m)

accuracy_rl <- sum(diag(confusion_m)) / sum(confusion_m)
print(accuracy_rl)
```

Obtenemos una precisión con gran similitud a la del conjunto de training, un 77,3%.
Para finalizar la regresión logística, vamos a graficar la curva ROC, la cual nos aporta la visualización de la relación entre los falsos y verdaderos positivos.
```{r}
library(ROCR)
pred_rl_roc <- prediction(as.numeric(pred_test), as.numeric(test_set$class))
perf_rl_roc <- performance(pred_rl_roc, "tpr", "fpr")
perf_rl_auc <- performance(pred_rl_roc, "auc")

print(perf_rl_auc@y.values[[1]])
plot(perf_rl_roc)
```
Observando la curva ROC resultante podemos comentar que se mantiene por encima de la diagonal, lo que es buena señal, pero se aproxima a ella, pudiendo haber proporcionado resultados mejores.

### k-NN<a name="knn"></a>

Antes de nada, para poder aplicar knn, debemos transformar las variables categóricas en numéricas.

```{r}
mushroom_num <- dummyVars(" ~ .", data = mushroom, fullRank = TRUE) %>% predict(mushroom)
mushroom_num <- as.data.frame(mushroom_num)
```

Visualizamos las variables categóricas codificadas y las dimensiones del dataset.
```{r}
dim_mushroom <- dim(mushroom_num)
print(dim_mushroom)
str(mushroom_num)
```
De nuevo dividimos los datos en conjunto de entrenamiento y de validación. En este caso 0 se corresponde con comestible y 1 con venenosa.
```{r}
library(caTools)
set.seed(18)

split <- sample.split(mushroom_num$class, SplitRatio = 0.8)
training_set_num <- subset(mushroom_num, split == TRUE)
test_set_num <- subset(mushroom_num, split == FALSE)

table(training_set_num$class)
table(test_set_num$class)
```

Para comenzar a aplicar el clasificador del vecino más cercano, haremos uso de la función knn() tomando como valor de k=5.

```{r}
library(class)
set.seed(18)
pred_knn <- knn(train = training_set_num[, -1], test = test_set_num[, -1], cl = training_set_num$class, k = 5)
summary(pred_knn)
```
Una vez tenemos los resultados de las predicciones, construimos la matriz de confusión.

```{r}
confusion_m <- table(test_set_num$class, pred_knn)
confusion_m

accuracy_knn <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_knn
```
Como resultado de la matriz de confusión, tenemos una precisión del 99%, resultando ser mejor que en la regresión logística aunque probablemente sobreajustado.

```{r}
library(ROCR)
pred_knn_roc <- prediction(as.numeric(pred_knn), as.numeric(test_set_num$class))
perf_knn_roc <- performance(pred_knn_roc, "tpr", "fpr")
perf_knn_auc <- performance(pred_knn_roc, "auc")

print(perf_knn_auc@y.values[[1]])
plot(perf_knn_roc)
```
Una vez obtenemos la curva ROC y el resultado del área de debajo de la misma, observamos que el resultado es muy positivo, ya que gráficamente se aleja de la diagonal.

### Clasificación con Árbol de Decisión<a name="arbol"></a>

Comenzamos aplicando el árbol de decisión mediante la función rpart() como se muestra a continuación.

```{r}
library(rpart)
set.seed(18)
dt_classiffier <- rpart(class ~ ., data = training_set)
```

A continuación vamos a visualizar el árbol de decisión normal y el árbol de decisión después de aplicar el pruning.
```{r}
library(rattle)
tree2 <- rpart(class ~ ., training_set,
  method = "class",
  control = rpart.control(cp = 0.00001)
)
fancyRpartPlot(tree2)

pruned <- prune(tree2, cp = 0.01)
fancyRpartPlot(pruned)
```
Construimos la matriz de confusión al igual que en los casos anteriores.

```{r}
pred_dt <- predict(dt_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_dt)
confusion_m

accuracy_dt <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_dt
```

Una vez tenemos el resultado para el valor de precisión en la predicción aplicando el árbol de decisión, 83%,
pasamos a aplicar de forma visual la construcción de la gráfica de la curva ROC y el cálculo del AUC. En este caso,
continúa siendo mejor resultado que el obtenido mediante la regresión logística, pero quedando por debajo del
resultado obtenido al aplicar KNN.

```{r}
library(ROCR)
pred_dt_roc <- prediction(as.numeric(pred_dt), as.numeric(test_set$class))
perf_dt_roc <- performance(pred_dt_roc, "tpr", "fpr")
perf_dt_auc <- performance(pred_dt_roc, "auc")

print(perf_dt_auc@y.values[[1]])
plot(perf_dt_roc)
```

### Clasificador Random Forest<a name="random"></a>

Comenzamos aplicando el clasificador de Random Forest mediante la función llamada de la misma forma; es decir, randomForest().
En esta función, el valor correspondiente al parámetro ntree indica la cantidad de árboles de decisión que formarán parte del clasificador.

```{r}
library(randomForest)
set.seed(18)
rf_classiffier <- randomForest(class ~ ., data = training_set, ntree = 250)
```

Realizamos las predicciones sobre el conjunto de datos y construimos la matriz de confusión.

```{r}
pred_rf <- predict(rf_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_rf)
confusion_m

accuracy_rf <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_rf
```

De forma similar al clasificador KNN, obtenemos un valor de precisión del 99%.
Definimos la curva ROC para este caso y realizamos el cálculo del área bajo la curva, los cuales nos simbolizan su altísima precisión probablemente causada por sobreajuste.

```{r}
library(ROCR)
pred_rf_roc <- prediction(as.numeric(pred_rf), as.numeric(test_set$class))
perf_rf_roc <- performance(pred_rf_roc, "tpr", "fpr")
perf_rf_auc <- performance(pred_rf_roc, "auc")

print(perf_rf_auc@y.values[[1]])
plot(perf_rf_roc)
```

### Kernel SVM Classifier

Para aplicar el clasificador Máquina de Soporte Vectorial hacemos uso de la función svm().
En esta función, los valores de los parámetros type y kernel hacen referencia al tipo de clasificador; es decir,
que el kernel es de tipo radial y gaussiano. 

```{r}
library(e1071)
set.seed(18)
svm_classiffier <- svm(class ~ .,
  data = training_set,
  type = "C-classification", kernel = "radial"
)
```
La predicción y la matríz de confusión son entonces las mostradas a continuación.

```{r}
pred_svm <- predict(svm_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_svm)
confusion_m

accuracy_svm <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_svm
```

Como se puede observar el valor de la predicción en este caso resulta ser del 95%, un resultado bueno que no muestra señales de sobreajuste.
Para finalizar, construimos la curva ROC correspondiente en este caso 

```{r}
library(ROCR)
pred_svm_roc <- prediction(as.numeric(pred_svm), as.numeric(test_set$class))
perf_svm_roc <- performance(pred_svm_roc, "tpr", "fpr")
perf_svm_auc <- performance(pred_svm_roc, "auc")

print(perf_svm_auc@y.values[[1]])
plot(perf_svm_roc)
```

### Conclusiones

Una vez aplicados los cinco distintos métodos de clasificación sobre nuestro dataset tras realizar el preprocesamiento, podemos concluir diciendo que los clasificadores de KNN y Random Forest son los que poseen mayor precisión y, por tanto, menor error de predicción. Sin embargo, con los resultados obtenidos por parte de ambos podemos detectar un posible sobreajuste, no beneficiando al modelo. Por esta razón, creemos que resulta más beneficioso sacrificar parte del valor de precisión, teniendo en cuenta algunos valores de falsos positivos y falsos negativos, como ocurre en el caso del clasificador de la Máquina de Soporte Vectorial, aprovechando así su capacidad de mayor generalización.

En las siguientes gráficas podemos comparar los niveles de precisión y AUC de los diferentes clasificadores.

```{r}
accuracy_comp <- matrix(c(accuracy_rl, accuracy_knn, accuracy_dt, accuracy_rf, accuracy_svm), ncol = 5)

barplot(accuracy_comp,
  main = "Accuracy Comparison",
  xlab = "Accuracy (%)",
  ylab = "Method",
  names.arg = c("RL", "K-NN", "DT", "RF", "SVM"),
  col = "#7fd6d9"
)
```

```{r}
perf_auc <- matrix(c(perf_rl_auc@y.values[[1]], perf_knn_auc@y.values[[1]], perf_dt_auc@y.values[[1]], perf_rf_auc@y.values[[1]], perf_svm_auc@y.values[[1]]), ncol = 5)

barplot(perf_auc,
  main = "AUC Comparison",
  xlab = "AUC (%)",
  ylab = "Method",
  names.arg = c("RL", "K-NN", "DT", "RF", "SVM"),
  col = "#7fd6d9"
)
```
## Análisis aprendizaje no supervisado<a name="unsupervised"></a>

En este apartado se analizará el dataset a través de algoritmos de aprendizaje no supervisado. En concreto, se probarán los algoritmos k-means y clustering jerárquico.
Para ambos algoritmos, se seguirá el siguiente esquema:

1. Representar la distribución inicial de los datos.
2. Determinar el número de clusters óptimo.
3. Representar la distribución de los datos en función del número de clusters.
4. Calcular el promedio de las variables en función del cluster al que pertenecen.
5. Calcular el accuracy del algoritmo.

Para poder trabajar con algoritmos no supervisados será necesario que las variables sean numéricas. Para ello, se eliminarán las variables categóricas del dataset.
```{r}
numerical_columns <- mushroom[, numerical_features]
```

### K-means<a name="kmeans"></a>

En primer lugar, representaremos de forma gráfica la distribución inicial de los datos.

```{r}
mushroom <- dummyVars(" ~ .", data = mushroom, fullRank = TRUE) %>% predict(mushroom)
mushroom <- as.data.frame(mushroom)
```

```{r}
df <- as.data.frame(numerical_columns)

plot_ly(df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width
) %>%
  add_markers(size = 1.5)
```

Para aplicar el algoritmo kmeans se utilizará la función kmeans() de la librería cluster. Será necesario determinar el número de clusters óptimo.
Para ello, se utilizará la función "kmeans" con diferentes valores de "centers" y se calculará la suma de cuadrados internos (within groups sum of squares) para cada valor de "centers". A continuación, se representará la suma de cuadrados internos vs. número de clusters.

```{r}
wss_per_k <- 0
for (i in 1:10) {
  kmeans_aux <- kmeans(numerical_columns, center = i, nstar = 20)
  wss_per_k[i] <- kmeans_aux$tot.withinss
}
par(mfrow = c(1, 1))
plot(1:10, wss_per_k,
  type = "b",
  xlab = "Number of clusters",
  ylab = "WSS",
)
```

Como se puede observar en la gráfica anterior, la suma de cuadrados internos disminuye a medida que aumenta el número de clusters. Sin embargo, a partir de 2 clusters, la disminución de la suma de cuadrados internos es muy pequeña. Por lo tanto, se decide utilizar 2 clusters.
Comprobamos que tiene sentido utilizar 2 clusters, ya que conocemos que el dataset es binario.

Generamos el modelo de k-means con 2 clusters.

```{r}
km_model <- kmeans(df, center = 2, nstar = 20)
```

Representamos la distribución de los datos en función de los clusters obtenidos.
```{r}
df$cluster <- factor(km_model$cluster)

plot_ly(df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width, color = ~cluster
) %>%
  add_markers(size = 1.5)
```

Se puede observar que los champiñones de menor tamaño (incluyendo diámetro, altura y anchura) pertenecen al cluster 2 y los de mayor tamaño pertenecen al cluster 1.

A continuación, calcularemos el valor promedio de las variables para cada cluster generado con el modelo de k-means.
```{r}
grouped_mushroom <- df %>%
  group_by(cluster) %>%
  summarise(
    mean_cap_diameter = mean(cap.diameter),
    mean_stem_height = mean(stem.height),
    mean_stem_width = mean(stem.width)
  )

grouped_mushroom
```

A partir de este momento, hemos decidido modificar el dataset actual debido a que para aplicar técnicas como "silhouette" o "dendrogram" (para el caso de clustering jerárquico) es necesario que el dataset sea de menor tamaño.
Para mantener la proporción de los datos de cada clase, se reducirá el dataset haciendo uso de createDataPartition, manteniendo únicamente un 1% de los datos iniciales para el análisis.

```{r}
set.seed(42)
split <- createDataPartition(mushroom$class, p = 0.01)
smaller_df <- mushroom[split$Resample1, ]
```

Comprobamos que la proporción de datos de cada clase se mantiene al hacer la partición.
```{r}
initial_class_prop <- table(mushroom$class) / nrow(mushroom)
smaller_class_prop <- table(smaller_df$class) / nrow(smaller_df)

print(initial_class_prop)
print(smaller_class_prop)
```

Ya podemos trabajar con el dataset reducido. Comprobamos que el dataset ahora cuenta con 611 ejemplos y 3 atributos.
```{r}
smaller_df <- smaller_df[, numerical_features]
dim(smaller_df)
```

En primer lugar, visualizaremos la distribución inicial de los datos a través de una gráfica 3D.

```{r}
plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width
) %>%
  add_markers(size = 1.5)
```

A continuación, vamos a estudiar cuál sería el número óptimo de clusters para el dataset reducido haciendo uso de la medida de bondad interna "silhouette". Para ello, utilizaremos la función fviz_nbclust de factoextra.
Silhouette es una medida que sirve para validar el número de clusters. Se calcula como la diferencia entre la distancia media de un punto a los puntos de su propio cluster y la distancia media de un punto a los puntos de su cluster más cercano.

```{r}
fviz_nbclust(smaller_df, FUNcluster = kmeans, method = "silhouette")
```

Según la gráfica, podemos afirmar que el número óptimo de clusters es 2.

Por último, visualizamos la distribución de los datos en función de los clusters obtenidos.

```{r}
km_sm_model <- kmeans(smaller_df, center = 2, nstart = 20)
cluster <- factor(km_sm_model$cluster)

plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width, color = ~cluster
) %>%
  add_markers(size = 1.5)
```

Por último, calculamos el valor promedio de las variables para cada cluster generado con el modelo de k-means.
```{r}
grouped_sm_mushroom <- smaller_df %>%
  mutate(cluster = cluster) %>%
  group_by(cluster) %>%
  summarise(
    mean_cap_diameter = mean(cap.diameter),
    mean_stem_height = mean(stem.height),
    mean_stem_width = mean(stem.width)
  )
grouped_sm_mushroom
```

### Clustering jerárquico<a name="hierarchical"></a>

A continuación, vamos a aplicar el algoritmo de clustering jerárquico a nuestro dataset reducido. Para ello, utilizaremos la función hclust. Primero, calculamos la distancia entre los puntos del dataset.
```{r}
distance <- dist(smaller_df)
hc_model <- hclust(distance)
```

Representamos el dendrograma para visualizar la distribución de los datos en función de los clusters obtenidos.

```{r}
dend_modelo <- as.dendrogram(hc_model)
plot(dend_modelo)
```

Hasta ahora, hemos obtenido la jerarquía de los datos, pero lo que realmente nos interesa es la clasificación de los datos en función de los clusters.
Cortaremos el dendrograma en un punto que nos interese para obtener los clusters. En este caso, hemos decidido cortar el dendrograma en 90 para obtener una visualización del dendograma cortado.
```{r}
cut <- 0.9

dend_modelo %>%
  color_branches(h = cut) %>%
  color_labels(h = cut) %>%
  plot()
```

Para obtener el número óptimo de cluster, haremos uso de la medida interna de bondad silhouette. Para ello, utilizaremos la función fviz_nbclust de factoextra.

```{r}
fviz_nbclust(smaller_df, FUNcluster = hcut, method = "silhouette")
```

Comprobamos que en este caso, el número óptimo de clusters podría ser 2 o 3, ya que el valor de silhouette es muy similar para ambos casos. En este caso, hemos decidido utilizar 2 clusters para poder comparar posteriormente los resultados con los obtenidos con el algoritmo de k-means.

Calculamos la agrupación del modelo en función del número de clusters que hemos decidido utilizar. Además, calculamos el promedio de los datos de cada cluster para ver si podemos sacar alguna conclusión.

```{r}
jq_cluster <- cutree(hc_model, k = 2)

grouped_mushroom <- smaller_df %>%
  mutate(cluster = jq_cluster) %>%
  group_by(cluster) %>%
  summarise_all(mean)
grouped_mushroom
```

Visualizamos la agrupación de los datos en función de los clusters obtenidos.

```{r}
jq_cluster <- factor(jq_cluster)

plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width,
  color = ~jq_cluster
) %>%
  add_markers(size = 1.5)
```

Con el objetivo de comparar los resultados obtenidos en los dos algoritmos, vamos a calcular el rendimiento de cada uno de ellos, haciendo uso del accuracy como medida de bondad externa.

En primer lugar, calculamos el accuracy del modelo de k-means. Supondremos que la clase 1 es la clase "e" y la clase 2 es la clase "p".
Para ello, obtenemos las clases reales y las clases predichas, y calculamos el accuracy.

Volvemos a obtener el dataset reducido para poder tener las clases reales.
```{r}
smaller_df <- mushroom[split$Resample1, ]
```

```{r}
real_classes <- ifelse(smaller_df$class == "e", 1, 2)
predicted_classes <- km_sm_model$cluster
predicted_classes <- as.numeric(predicted_classes)
```

```{r}
accuracy <- sum(real_classes == predicted_classes) / length(real_classes)
print(accuracy)
```

Hacemos lo mismo con el modelo de clustering jerárquico, pero en este caso, supondremos que la clase 1 es la clase "p" y la clase 2 es la clase "e".
```{r}
real_classes <- ifelse(smaller_df$class == "e", 2, 1)
predicted_classes <- as.numeric(jq_cluster)
```

```{r}
accuracy <- sum(real_classes == predicted_classes) / length(real_classes)
print(accuracy)
```

Tras comparar los resultados obtenidos en los dos algoritmos, podemos afirmar que el modelo de clustering jerárquico ha obtenido un accuracy mayor para este dataset.
