---
title: "Mushroom Data Analysis"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---

# Índice   
1. [Introducción](#introduction) \
2. [Cargar librerías](#libraries) \
3. [Cargar datos](#data) \
4. [Visualización y preprocesamiento](#preprocessing) \
4.1 [Imputación de valores nulos](#imputation) \
4.2 [Codificación de variables categóricas](#encoding) \
4.3 [Escalado de variables numéricas](#scaling)
5. [Análisis de datos](#analysis) \
5.1 [Análisis aprendizaje supervisado](#supervised) \
5.2 [Análisis aprendizaje no supervisado](#unsupervised) \
5.2.1 [K-means](#kmeans) \
5.2.2 [Clustering jerárquico](#hierarchical) \

\pagebreak

# Introducción<a name="introduction"></a>

Este trabajo se centra en el análisis de un conjunto de datos que contiene información sobre 61069 champiñones diferentes. Los datos han sido obtenidos de Kaggle y cada una de las instancias incluye 21 variables que describen diferentes aspectos de los champiñones, como su forma, tamaño y hábitat. En estas variables se encuentra inlcuida la clase de cada champiñón, indicando si es venenoso o comestible.
Antes de comenzar el análisis, es necesario realizar una fase de preprocesamiento de los datos. Esta fase tiene como objetivo preparar los datos para su análisis y obtener mayor conocimiento sobre estos. Durante esta fase, realizaremos tareas como visualizar los datos, detectar y tratar valores faltantes, o normalizar los datos.
Una vez preparados los datos, procederemos a realizar el análisis. Para ello, utilizaremos tanto técnicas de aprendizaje supervisado como no supervisado. El aprendizaje supervisado implica entrenar un modelo con datos etiquetados, es decir, que ya conocemos la clase de cada instancia. Una vez entrenado el modelo, podemos utilizarlo para predecir la clase de nuevas instancias. En este caso, utilizaremos diferentes algoritmos de clasificación para evaluar diferentes modelos.
El aprendizaje no supervisado, por otro lado, no requiere de datos etiquetados. En este caso, el objetivo es agrupar las instancias en diferentes grupos de forma que las instancias de un mismo grupo sean similares entre sí y diferentes a las de los demás grupos. Utilizaremos dos algoritmos de clustering para evaluar diferentes modelos. Además, cabe destacar, que en nuestro caso contamos con un conocimiento a priori de los datos, ya que conocemos la clase de cada instancia. Por tanto, podemos utilizar este conocimiento para evaluar los modelos de clustering.

A continuación se detalla los posibles valores que pueden tomar las variables del dataset:
**1. class:** edible=e, poisonous=p \
**2. cap-diameter:** float number in cm \
**3. cap-shape:** bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s \
**4. cap-surface:** fibrous=f, grooves=g, scaly=y, smooth=s \
**5. cap-color:** brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y \
**6. does.bruise.or.bleed:** bruises-or-bleeding=t,no=f
**7. gill-attachment:** attached=a, descending=d, free=f, notched=n \
**8. gill-spacing:** close=c, crowded=w, distant=d \
**9. gill-color:** black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \
**10. stem-height:** float number in cm \
**11. stem-width:** float number in mm  \
**12. stem-root:** bulbous=b, swollen=s, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r \
**13. stem-surface:** see cap-surface + none=f \
**14. stem-color:** see cap-color + none=f \
**15. veil-type:** partial=p, universal=u \
**16. veil-color:** see cap-color + none=f \
**17. has-ring:** ring=t, none=f \
**18. ring-type:** cobwebby=c, evanescent=e, flaring=r, grooved=g, large=l, pendant=p, sheathing=s, zone=z, scaly=y, movable=m, none=f, unknown=? \
**19. spore-print-color:** see cap color \
**20. habitat:** grasses=g, leaves=l, meadows=m, paths=p, heaths=h, urban=u, waste=w, woods=d \
**21. season:** spring=s, summer=u, autumn=a, winter=w \

# Autores<a name="authors"></a>
Este trabajo ha sido realizado por: 
- Ana Diaz
- Deyan Rosenov
- Javier Vilariño
- María Isabel Ramos

Todos los ingrantes del grupo hemos realizado de forma conjunta la visualización y el preprocesamiento de los datos. Por otro lado, Ana y Deyan se ha encargado de realizar el aprendizaje supervisado, mientras que Javier y María Isabel se han encargado de realizar el aprendizaje no supervisado.


# Cargar librerías<a name="libraries"></a>

En los siguientes fragmentos se incluye el código necesario para instalar y cargar las librerías necesarias para el análisis de los datos realizado.
```{r}
#install.packages("caret")
#install.packages("tidyverse")
#install.packages("plotly")
#install.packages("dplyr")
#install.packages("factoextra")
#install.packages("dendextend")
```
```{r}
library(caret)
library(tidyverse)
library(plotly)
library(dplyr)
library(cluster)
library(factoextra)
library(dendextend)
```

# Cargar datos<a name="data"></a>

En primer lugar, realizamos la lectura del fichero csv, separando las columnas por ";" y mostramos las primeras 6 filas del fichero. Este código nos permitirá acceder a los datos de champiñones y trabajar con ellos en nuestro código R.
```{r}
mushroom <- read.csv("./data/data.csv", sep = ";")
head(mushroom)
```

Echamos un vistazo a las características de los atributos del dataset. En el caso de las variables numéricas, se puede observar valores como el mínimo, máximo, media, desviación estándar, etc. Por otro lado, en el caso de las variables categóricas no obtenemos información relevante.
```{r}
summary(mushroom)
```

# Visualización y preprocesamiento<a name="preprocessing"></a>

En primer lugar, vamos a comprobar si existen valores nulos en el dataset. Para ello, utilizaremos la función colSums(is.na(mushroom)), que nos devolverá la suma de valores nulos de cada variable.
```{r}
colSums(is.na(mushroom))
```

Según el resultado obtenido anteriormente, no existen valores nulos en el dataset. Sin embargo, si observamos el dataset, podemos observar que existen valores vacíos. Para poder trabajar con ellos, y que no nos de problemas a la hora de realizar el preprocesamiento, sustituiremos dichos valores vacíos por NA.
```{r}
mushroom[mushroom == ""] <- NA
```

Comprobamos que se han sustituido correctamente los valores vacíos por NA, y ahora si podemos comprobar la cantidad de valores nulos que hay en cada variable.
Esta información nos ayudará a decidir si será conveniente eliminar dichas variables o no.
```{r}
colSums(is.na(mushroom))
```
Podemos observar que existen 5 variables donde más del 50% de los valores son nulos. Estas son: stem.surface, veil.color, spore.print.color, stem.root, veil.type.

En este caso, decidimos eliminar aquellas variables que tenían más del 50% de sus valores faltantes. La razón de esta decisión es que, si decidimos imputar (es decir, reemplazar) los valores faltantes, estaríamos inventando demasiados datos. Imputar valores faltantes significa reemplazar los valores faltantes con algún valor que estimemos adecuado. Sin embargo, si una variable tiene más del 50% de sus valores faltantes, significa que estaríamos reemplazando más de la mitad de los valores de esa variable. Esto nos llevaría a tener un conjunto de datos con demasiados valores inventados, lo que podría afectar la precisión de nuestro análisis.
Para ello, en primer lugar, obtendremos el nombre de las columnas que queremos eliminar.

```{r}
nacols <- colnames(mushroom)[colSums(is.na(mushroom)) > nrow(mushroom) / 2]
print(nacols)
```

A continuación, eliminaremos dichas columnas del dataset.
```{r}
mushroom <- mushroom[, !names(mushroom) %in% nacols]
```

Comprobamos que se han eliminado correctamente las columnas.
```{r}
print(colnames(mushroom))
```

Para poder analizar de forma específica cada variable, separamos las variables numéricas de las categóricas.
```{r}
colsnames <- colnames(mushroom)
numerical_features <- c("cap.diameter", "stem.height", "stem.width")
categorical_features <- colsnames[!colsnames %in% numerical_features]
print(categorical_features)
print(numerical_features)
```

Comenzamos analizando las variables categóricas. Para ello visualizamos la distribución de las variables categóricas a través de histogramas. Observaremos los posibles valores de cada variable categórica junto con su frecuencia de aparición en el dataset.
```{r}
for (i in categorical_features) {
  print(ggplot(mushroom, aes_string(x = i)) +
    geom_bar(fill = "#7fd6d9") +
    geom_text(stat = "count", aes(label = scales::percent(..count.. / nrow(mushroom)), vjust = -0.25)) +
    labs(x = i, y = "Percentage") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)))
}
```

Tras visualizar los histogramas de cada variable, obtenemos a las siguientes conclusiones:

* La variable dependiente "class" está balanceada, es decir, la frecuencia de aparición de "e" (edible) y "p" (poisonous) es similar, en una proporción de 44,5% y 55,5% respectivamente. Por lo tanto, no es necesario realizar un balanceo de la variable dependiente, permitiendo aplicar la medida de evaluación "accuracy" para evaluar el modelo.
* Siguen existiendo valores NA en algunas variables, pero estos serán imputados posteriormente con la moda de cada variable.
* La variable ring-type tiene 8 posibles valores, pero en casi un 80% de los casos, el valor es "f", por lo que quizás se podría eliminar esta variable en un futuro, ya que parece que aporta información relevante. Más adelante la analizaremos en profundidad.

A continuación, seguiremos con el análisis de las variables numéricas, donde visualizaremos su distribución respecto a la clase a través de una gráfica generada con "featurePlot". Esta requiere que la variable objetivo sea de tipo factor, por lo que hacemos la conversión.
```{r}
mushroom$class <- as.factor(mushroom$class)
featurePlot(x = mushroom[, numerical_features], y = mushroom$class, plot = "strip")
```
Con la gráfica anterior, se puede observar que para valores altos de cap.diameter, stem.height y stem.width, la probabilidad de que la clase sea "e" es mayor que la de "p". Por lo tanto, podemos deducir que si una seta es de tamaño grande, su probabilidad de ser comestible es bastante mayor.


Anteriormente hemos comentado que la variable "ring.type" tiene 8 posibles valores, pero en casi un 80% de los casos, el valor es "f". Para evaluar su posible eliminación o la de alguna otra variable, se puede utilizar la función "nearZeroVar". Esta función devuelve un vector con los índices de las variables que tienen una varianza cercana a 0.
```{r}
near_zero_col <- nearZeroVar(mushroom, saveMetrics = FALSE)
colnames(mushroom)[c(near_zero_col)]
```

Como suponíamos, la variable "ring.type" es la que tiene una varianza cercana a 0, y por tanto podría ser eliminada. A continuación, visualizaremos la correlación existente con la variable dependiente "class" para tomar una decisión.
```{r}
print(ggplot(mushroom, aes_string(x = "ring.type")) +
  geom_bar(aes(fill = class)))
```

Tras visualizar la gráfica anterior, se puede observar que la distribución de la variable dependiente "class" es similar en casi todos los valores de "ring.type". Sin embargo, en el caso de "ring.type" = "z" y "ring.type" = "m", la distribución de la variable dependiente "class" es diferente. Por lo tanto, se decide finalemnte mantener la variable "ring.type".

## Imputación de valores nulos<a name="imputation"></a>

Como se ha comentado anteriormente, los valores nulos de las variables categóricas se imputarán a través de la moda de cada variable. 
Este proceso lo hacemos de forma manual, ya que la función preProcess() de la librería caret no permite imputar valores nulos de variables categóricas.
```{r}
for (i in categorical_features) {
  mushroom[, i][is.na(mushroom[, i])] <- names(which.max(table(mushroom[, i])))
}
```

Comprobamos que ya no existen valores nulos en las variables categóricas.
```{r}
colSums(is.na(mushroom))
```

## Escalado de variables numéricas<a name="scaling"></a>

Con el objetivo de que todas las variables tengan la misma escala para evitar que una variable tenga más peso que otra, se escalarán las variables numéricas.
Para realizar el escalado las variables numéricas, se utilizará la función preProcess() de la librería caret. Esta función devuelve un objeto de tipo "preProcess" que contiene la información necesaria para escalar las variables numéricas. 
A continuación, se realizara el escalado y se sustituiran las variables numéricas originales por las escaladas.
```{r}
range_numeric <- preProcess(mushroom[, numerical_features], method = c("range"))
mushroom[, numerical_features] <- predict(range_numeric, newdata = mushroom[, numerical_features])
str(mushroom)
```


# Análisis de datos<a name="analysis"></a>


## Análisis aprendizaje supervisado<a name="supervised"></a>

Para trabajar con el dataset mediante el aprendizaje supervisado; es decir, utilizando datos que son etiquetados mediante la intervención de un ser humano, utilizaremos diferentes tipos de clasificadores, algunos de ellos ya trabajados durante las clases prácticas de la asignatura y otros que eran desconocidos para nosotros y sobre los cuales hemos tenido que investigar anteriormente sobre su funcionamiento en R.
Los algoritmos de clasificación que hemos seleccionado y los cuales vamos a aplicar son:
  1. Regresión logística.
  2. KNN o el Vecino más cercano.
  3. Árboles de decisión.
  4. Random Forest.
  5. MSV o Máquina de Soporte Vectorial.
Una vez aplicados cada uno de los clasificadores, los compararemos entre sí y seleccionaremos el o los algoritmos que mayor precisión proporcionen sin llegar al sobreajuste, tratando de buscar que el resultado final sea generalizado para los datos.


En primer lugar, ante de comenzar a aplicar los clasificadores, dividiremos el dataset en dos conjuntos: el primer conjunto será el de entrenamiento o training y el segundo será el conjunto de prueba o test. El conjunto de entrenamiento lo utilizaremos para entrenar los distintos modelos y el conjunto de prueba lo utilizaremos para evaluar cada uno de ellos una vez obtenidos.

```{r}
library(caTools)
set.seed(18)

split <- sample.split(mushroom$class, SplitRatio = 0.8)
training_set <- subset(mushroom, split == TRUE)
test_set <- subset(mushroom, split == FALSE)

table(training_set$class)
table(test_set$class)
```


### Regresión Logística<a name="logistic"></a>

La regresión logística permite predecir el resultado de una variable categórica en función de las variables independientes o predictoras.
A continuación se muestra un resumen del conjunto de datos de entrenamiento con el cual vamos a trabajar una vez aplicada la función glm().

```{r}
rl_classiffier <- glm(class ~ ., family = binomial, data = training_set)
summary(rl_classiffier)
```

La función aplicada, glm(), obtiene los valores residuales del modelo y los coeficientes de ajuste para cada una de las variables independientes. Además, se obtiene el p-value correspondiente para cada una de ellas.
En el resumen mostrado mediante la función summary(), se pueden observar variables con dos o tres asteriscos, las cuales aportan bastante relevancia al modelo como predictores; sin embargo, las variables que poseen un solo asterisco o incluso ninguno, significa que apenas aportan relevancia a los resultados.
A continuación, procedemos a predecir las clases del conjunto de entrenamiento y de validación. Tomando como umbral '0,5', dividiremos los champiñones en comestibles y venenosos, de forma que si la probabilidad queda por encima de dicho umbral significará que el champiñón es comestible, y de lo contrario, si el resultado se mantiene por debajo, significará que el champiñón es venenoso. Para acabar, formaremos la matriz de confusión con los resultados de las predicciones para proceder a su análisis y valoración.

```{r}
pred_train <- predict(rl_classiffier, newdata = training_set, type = "response")
pred_train <- ifelse(pred_train > 0.5, "p", "e")
pred_train <- factor(pred_train, levels = c("e", "p"), labels = c("e", "p"))

confusion_m <- table(training_set$class, pred_train)
print(confusion_m)

accuracy <- sum(diag(confusion_m)) / sum(confusion_m)
print(accuracy)
```


Una vez obtenido el resultado mostrado mediante la matriz de confusión, se observa que la predicción posee una precisión del 77,8%.
A continuación, se muestra el resultado de aplicar el mismo proceso anterior sobre el conjunto de prueba.

```{r}
pred_test <- predict(rl_classiffier, newdata = test_set, type = "response")
pred_test <- ifelse(pred_test > 0.5, "p", "e")
pred_test <- factor(pred_test, levels = c("e", "p"), labels = c("e", "p"))

confusion_m <- table(test_set$class, pred_test)
print(confusion_m)

accuracy_rl <- sum(diag(confusion_m)) / sum(confusion_m)
print(accuracy_rl)
```

Una vez obtenido este segundo resultado, podemos observar que hay una gran similitud entre los resultados obtenidos para ambos conjuntos de datos, resultando esta vez en un 77,3%.
Antes de acabar de aplicar el clasificador de regresión logística, vamos a proceder a graficar la curva ROC, la cual nos aporta mayor visualización de la relación entre los falsos y verdaderos positivos.

```{r}
library(ROCR)
pred_rl_roc <- prediction(as.numeric(pred_test), as.numeric(test_set$class))
perf_rl_roc <- performance(pred_rl_roc, "tpr", "fpr")
perf_rl_auc <- performance(pred_rl_roc, "auc")

print(perf_rl_auc@y.values[[1]])
plot(perf_rl_roc, col = "lightblue", lwd = 5)  
```

Observando la curva ROC resultante podemos comentar que se mantiene por encima de la diagonal, lo que es buena señal, pero se aproxima a ella, pudiendo haber proporcionado resultados mejores resulta ser un modelo bastante generalizado.


### k-NN<a name="knn"></a>

Mediante el algoritmo de clasificación llamado k-NN se asigna una nueva observación a la clase más común entre sus "k" vecinos más cercanos en el espacio de características.
Antes de nada, para poder aplicar el algoritmo k-NN, debemos transformar las variables categóricas en numéricas.

```{r}
mushroom_num <- dummyVars(" ~ .", data = mushroom, fullRank = TRUE) %>% predict(mushroom)
mushroom_num <- as.data.frame(mushroom_num)
```

A continuación, se visualizan las variables categóricas ya codificadas y las dimensiones del dataset.

```{r}
dim_mushroom <- dim(mushroom_num)
print(dim_mushroom)
str(mushroom_num)
```

Debido al hecho de necesitar la transformación de las variables categóricas en numéricas, procedemos en este paso a dividir los datos en los conjuntos de entrenamiento y de validación. En este caso, el valor 0 corresponde con los champiñones comestibles y el valor 1 con los champiñones venenosos.

```{r}
library(caTools)
set.seed(18)

split <- sample.split(mushroom_num$class, SplitRatio = 0.8)
training_set_num <- subset(mushroom_num, split == TRUE)
test_set_num <- subset(mushroom_num, split == FALSE)

table(training_set_num$class)
table(test_set_num$class)
```

El siguiente paso será determinar el valor óptimo de k antes de proceder a aplicar la función. Para ello, utilizaremos el resultado que nos proporciona el cálculo de la raíz cuadrada del número de observaciones del conjunto de entrenamiento.

```{r}

nrows_class <- NROW(training_set_num) 
k <- sqrt(nrows_class)
k <- round(k)
k
```

Una vez hemos obtenido el valor de k, procedemos a realizar las predicciones. Para llevar a cabo la aplicación del clasificador llamado 'el vecino más cercano', haremos uso de la función knn() de la librería class.

```{r}
library(class)
set.seed(18)
pred_knn <- knn(train = training_set_num[, -1], test = test_set_num[, -1], cl = training_set_num$class, k = k)
summary(pred_knn)
```

Una vez obtenidos los resultados de las predicciones en este caso, podemos de igual forma construir la matriz de confusión.

```{r}
confusion_m <- table(test_set_num$class, pred_knn)
confusion_m

accuracy_knn <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_knn
```

Como resultado final de la matriz de confusión, obtenemos una precisión resultante del 97%, la cual mejora respecto al algoritmo de clasificación anterior, aunque tendiendo a poseer mayor sobreajuste.
De forma más visual, obtenemos a continuación la gráfica de la curva ROC y el cálculo del área bajo la misma.

```{r}
library(ROCR)
pred_knn_roc <- prediction(as.numeric(pred_knn), as.numeric(test_set_num$class))
perf_knn_roc <- performance(pred_knn_roc, "tpr", "fpr")
perf_knn_auc <- performance(pred_knn_roc, "auc")

print(perf_knn_auc@y.values[[1]])
plot(perf_knn_roc, col = "lightblue", lwd = 5)  
```

Una vez obtenemos la curva ROC y su área, observamos que el resultado es muy positivo en cuanto a precisión de los resultados, ya que como se puede observar gráficamente se aleja de la diagonal.


### Clasificación con Árbol de Decisión<a name="arbol"></a>

Los árboles de decisión se basan en la construcción de reglas lógicas (divisiones de los datos entre rangos o condiciones) a partir de los datos de entrada.
Para trabajar con este clasificador comenzamos aplicando la función del árbol de decisión, rpart(), sobre el conjunto de datos de entrenamiento como se muestra a continuación.

```{r}
library(rpart)
set.seed(18)
dt_classiffier <- rpart(class ~ ., data = training_set)
```
Una vez obtenido el resultado, lo graficaremos para facilitar así el análisis del resultado.

```{r}
library(rpart.plot)
rpart.plot(dt_classiffier)
```

Construimos la matriz de confusión con los resultados obtenidos anteriormente para este caso.

```{r}
pred_dt <- predict(dt_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_dt)
confusion_m

accuracy_dt <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_dt
```
Una vez tenemos el resultado para el valor de precisión en la predicción aplicando el árbol de decisión, 83%, procedemos a construir la gráfica de la curva ROC y a calcular el valor correspondiente al AUC. En este caso, continúa siendo mejor resultado que el obtenido mediante la regresión logística, puesto que resulta en una mayor precisión, pero al igual que el algoritmo de k-NN, tiende a ser más sobreajustado.

```{r}
library(ROCR)
pred_dt_roc <- prediction(as.numeric(pred_dt), as.numeric(test_set$class))
perf_dt_roc <- performance(pred_dt_roc, "tpr", "fpr")
perf_dt_auc <- performance(pred_dt_roc, "auc")

print(perf_dt_auc@y.values[[1]])
plot(perf_dt_roc, col = "lightblue", lwd = 5)  
```


### Clasificador Random Forest<a name="random"></a>

El algoritmo de Random Forest trabaja mediante la combinación de árboles predictores tal que cada árbol depende de los valores de un vector aleatorio.
Comenzamos aplicando dicho clasificador mediante la función llamada de la misma forma; es decir, randomForest(). En esta función, el valor correspondiente al parámetro llamado 'ntree' indica la cantidad de árboles de decisión que formarán parte del clasificador.

```{r}
library(randomForest)
set.seed(18)
rf_classiffier <- randomForest(class ~ ., data = training_set, ntree = 250)
```

Mediante su gráfica, vamos a proceder a comparar los errores en función del aumento del número de árboles; es decir, cuanto más vaya aumentando el número de árboles hasta un umbral determinado, menor cantidad de errores poseerá la predicción.

```{r}
plot(rf_classiffier)
```

Calculamos las predicciones sobre el conjunto de datos de prueba y construimos la matriz de confusión.

```{r}
pred_rf <- predict(rf_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_rf)
confusion_m

accuracy_rf <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_rf
```

Una vez obtenido el valor de la precisión para este caso, definimos la curva ROC y procedemos a realizar el cálculo del área bajo la curva. Mediante este último paso se puede observar su tan alta precisión, la cual indica demasiado sobreajuste sin ser conveniente.

```{r}
library(ROCR)
pred_rf_roc <- prediction(as.numeric(pred_rf), as.numeric(test_set$class))
perf_rf_roc <- performance(pred_rf_roc, "tpr", "fpr")
perf_rf_auc <- performance(pred_rf_roc, "auc")

print(perf_rf_auc@y.values[[1]])
plot(perf_rf_roc, col = "lightblue", lwd = 5)  
```


### Kernel SVM Classifier

El clasificador de la máquina vectorial, encuentra la curva que es capaz de separar y clasificar los datos de entrenamiento garantizando que la separación entre ésta y ciertas observaciones del conjunto de entrenamiento resulte ser lo mayor posible.
Para llevar a cabo la aplicación del clasificador llamado 'Máquina de Soporte Vectorial' hacemos uso de la función svm(). En dicha función, los valores de los parámetros 'type' y 'kernel' hacen referencia al tipo de clasificador lo que significa que el kernel será de tipo radial y gaussiano. 

```{r}
library(e1071)
set.seed(18)
svm_classiffier <- svm(class ~ .,
  data = training_set,
  type = "C-classification", kernel = "radial"
)
```
A continuación, se calcula la predicción y se construye la matríz de confusión, las cuales resultan ser las siguientes.

```{r}
pred_svm <- predict(svm_classiffier, newdata = test_set, type = "class")

confusion_m <- table(test_set$class, pred_svm)
confusion_m

accuracy_svm <- sum(diag(confusion_m)) / sum(confusion_m)
accuracy_svm
```

Como se puede observar, el valor de la precisión en la predicción en este caso resulta ser del 95%, un resultado bueno que no muestra señales de sobreajuste.
Para finalizar, construimos la curva ROC correspondiente en este caso y calculamos su área.

```{r}
library(ROCR)
pred_svm_roc <- prediction(as.numeric(pred_svm), as.numeric(test_set$class))
perf_svm_roc <- performance(pred_svm_roc, "tpr", "fpr")
perf_svm_auc <- performance(pred_svm_roc, "auc")

print(perf_svm_auc@y.values[[1]])
plot(perf_svm_roc, col = "lightblue", lwd = 5)  
```


### Conclusiones

Una vez aplicados los cinco distintos métodos de clasificación sobre nuestro dataset llamado 'mushroom' tras haber realizado antes su preprocesamiento, podemos concluir diciendo que el clasificador de Random Forest es el que ha resultado poseer un mayor valor en la precisión de la predicción en la clasificación y, por lo tanto, un menor valor para el error de predicción. Sin embargo, al aplicar este algoritmo hemos obtenido un mayor sobreajuste, el cual no beneficia al modelo ya que se busca que los resultados obtenidos sean precisos, pero también generalizados para los datos. Por esta razón, consideramos que resulta más beneficioso sacrificar parte del valor de precisión, teniendo en cuenta algunos valores de falsos positivos y falsos negativos, como ocurre en el caso de los clasificadores de la Máquina de Soporte Vectorial o k-NN, aprovechando así su capacidad de mayor generalización.
Sobre los gráficos que se muestran a continuación se pueden comparar los distintos niveles de precisión y AUC para cada uno de los diferentes clasificadores que han sido aplicados.

```{r}
accuracy_comp <- matrix(c(accuracy_rl, accuracy_knn, accuracy_dt, accuracy_rf, accuracy_svm), ncol = 5)

barplot(accuracy_comp,
  main = "Accuracy Comparison",
  xlab = "Accuracy (%)",
  ylab = "Method",
  names.arg = c("RL", "K-NN", "DT", "RF", "SVM"),
  col = "#7fd6d9"
)
```

```{r}
perf_auc <- matrix(c(perf_rl_auc@y.values[[1]], perf_knn_auc@y.values[[1]], perf_dt_auc@y.values[[1]], perf_rf_auc@y.values[[1]], perf_svm_auc@y.values[[1]]), ncol = 5)

barplot(perf_auc,
  main = "AUC Comparison",
  xlab = "AUC (%)",
  ylab = "Method",
  names.arg = c("RL", "K-NN", "DT", "RF", "SVM"),
  col = "#7fd6d9"
)
```


## Análisis aprendizaje no supervisado<a name="unsupervised"></a>

En este apartado se analizará el dataset a través de algoritmos de aprendizaje no supervisado. En concreto, se probarán los algoritmos k-means y clustering jerárquico.
Para ambos algoritmos, se seguirá el siguiente esquema:

1. Representar la distribución inicial de los datos.
2. Determinar el número de clusters óptimo.
3. Representar la distribución de los datos en función del número de clusters.
4. Calcular el promedio de las variables en función del cluster al que pertenecen.
5. Calcular el accuracy del algoritmo.

Para poder trabajar con algoritmos no supervisados será necesario que las variables sean numéricas. Para ello, se eliminarán las variables categóricas del dataset.
```{r}
numerical_columns <- mushroom[, numerical_features]
```

### K-means<a name="kmeans"></a>

En primer lugar, representaremos de forma gráfica la distribución inicial de los datos.

```{r}
mushroom <- dummyVars(" ~ .", data = mushroom, fullRank = TRUE) %>% predict(mushroom)
mushroom <- as.data.frame(mushroom)
```

```{r}
df <- as.data.frame(numerical_columns)

plot_ly(df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width
) %>%
  add_markers(size = 1.5)
```

Para aplicar el algoritmo kmeans se utilizará la función kmeans() de la librería cluster. Será necesario determinar el número de clusters óptimo.
Para ello, se utilizará la función "kmeans" con diferentes valores de "centers" y se calculará la suma de cuadrados internos (within groups sum of squares) para cada valor de "centers". A continuación, se representará la suma de cuadrados internos vs. número de clusters.

```{r}
wss_per_k <- 0
for (i in 1:10) {
  kmeans_aux <- kmeans(numerical_columns, center = i, nstar = 20)
  wss_per_k[i] <- kmeans_aux$tot.withinss
}
par(mfrow = c(1, 1))
plot(1:10, wss_per_k,
  type = "b",
  xlab = "Number of clusters",
  ylab = "WSS",
)
```

Como se puede observar en la gráfica anterior, la suma de cuadrados internos disminuye a medida que aumenta el número de clusters. Sin embargo, a partir de 2 clusters, la disminución de la suma de cuadrados internos es muy pequeña. Por lo tanto, se decide utilizar 2 clusters.
Comprobamos que tiene sentido utilizar 2 clusters, ya que conocemos que el dataset es binario.

Generamos el modelo de k-means con 2 clusters.

```{r}
km_model <- kmeans(df, center = 2, nstar = 20)
```

Representamos la distribución de los datos en función de los clusters obtenidos.
```{r}
df$cluster <- factor(km_model$cluster)

plot_ly(df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width, color = ~cluster
) %>%
  add_markers(size = 1.5)
```

Se puede observar que los champiñones de menor tamaño (incluyendo diámetro, altura y anchura) pertenecen al cluster 2 y los de mayor tamaño pertenecen al cluster 1.

A continuación, calcularemos el valor promedio de las variables para cada cluster generado con el modelo de k-means.
```{r}
grouped_mushroom <- df %>%
  group_by(cluster) %>%
  summarise(
    mean_cap_diameter = mean(cap.diameter),
    mean_stem_height = mean(stem.height),
    mean_stem_width = mean(stem.width)
  )

grouped_mushroom
```

A partir de este momento, hemos decidido modificar el dataset actual debido a que para aplicar técnicas como "silhouette" o "dendrogram" (para el caso de clustering jerárquico) es necesario que el dataset sea de menor tamaño.
Para mantener la proporción de los datos de cada clase, se reducirá el dataset haciendo uso de createDataPartition, manteniendo únicamente un 1% de los datos iniciales para el análisis.

```{r}
set.seed(42)
split <- createDataPartition(mushroom$class, p = 0.01)
smaller_df <- mushroom[split$Resample1, ]
```

Comprobamos que la proporción de datos de cada clase se mantiene al hacer la partición.
```{r}
initial_class_prop <- table(mushroom$class) / nrow(mushroom)
smaller_class_prop <- table(smaller_df$class) / nrow(smaller_df)

print(initial_class_prop)
print(smaller_class_prop)
```

Ya podemos trabajar con el dataset reducido. Comprobamos que el dataset ahora cuenta con 611 ejemplos y 3 atributos.
```{r}
smaller_df <- smaller_df[, numerical_features]
dim(smaller_df)
```

En primer lugar, visualizaremos la distribución inicial de los datos a través de una gráfica 3D.

```{r}
plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width
) %>%
  add_markers(size = 1.5)
```

A continuación, vamos a estudiar cuál sería el número óptimo de clusters para el dataset reducido haciendo uso de la medida de bondad interna "silhouette". Para ello, utilizaremos la función fviz_nbclust de factoextra.
Silhouette es una medida que sirve para validar el número de clusters. Se calcula como la diferencia entre la distancia media de un punto a los puntos de su propio cluster y la distancia media de un punto a los puntos de su cluster más cercano.

```{r}
fviz_nbclust(smaller_df, FUNcluster = kmeans, method = "silhouette")
```

Según la gráfica, podemos afirmar que el número óptimo de clusters es 2.

Por último, visualizamos la distribución de los datos en función de los clusters obtenidos.

```{r}
km_sm_model <- kmeans(smaller_df, center = 2, nstart = 20)
cluster <- factor(km_sm_model$cluster)

plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width, color = ~cluster
) %>%
  add_markers(size = 1.5)
```

Por último, calculamos el valor promedio de las variables para cada cluster generado con el modelo de k-means.
```{r}
grouped_sm_mushroom <- smaller_df %>%
  mutate(cluster = cluster) %>%
  group_by(cluster) %>%
  summarise(
    mean_cap_diameter = mean(cap.diameter),
    mean_stem_height = mean(stem.height),
    mean_stem_width = mean(stem.width)
  )
grouped_sm_mushroom
```

### Clustering jerárquico<a name="hierarchical"></a>

A continuación, vamos a aplicar el algoritmo de clustering jerárquico a nuestro dataset reducido. Para ello, utilizaremos la función hclust. Primero, calculamos la distancia entre los puntos del dataset.
```{r}
distance <- dist(smaller_df)
hc_model <- hclust(distance)
```

Representamos el dendrograma para visualizar la distribución de los datos en función de los clusters obtenidos.

```{r}
dend_modelo <- as.dendrogram(hc_model)
plot(dend_modelo)
```

Hasta ahora, hemos obtenido la jerarquía de los datos, pero lo que realmente nos interesa es la clasificación de los datos en función de los clusters.
Cortaremos el dendrograma en un punto que nos interese para obtener los clusters. En este caso, hemos decidido cortar el dendrograma en 90 para obtener una visualización del dendograma cortado.
```{r}
cut <- 0.9

dend_modelo %>%
  color_branches(h = cut) %>%
  color_labels(h = cut) %>%
  plot()
```

Para obtener el número óptimo de cluster, haremos uso de la medida interna de bondad silhouette. Para ello, utilizaremos la función fviz_nbclust de factoextra.

```{r}
fviz_nbclust(smaller_df, FUNcluster = hcut, method = "silhouette")
```

Comprobamos que en este caso, el número óptimo de clusters podría ser 2 o 3, ya que el valor de silhouette es muy similar para ambos casos. En este caso, hemos decidido utilizar 2 clusters para poder comparar posteriormente los resultados con los obtenidos con el algoritmo de k-means.

Calculamos la agrupación del modelo en función del número de clusters que hemos decidido utilizar. Además, calculamos el promedio de los datos de cada cluster para ver si podemos sacar alguna conclusión.

```{r}
jq_cluster <- cutree(hc_model, k = 2)

grouped_mushroom <- smaller_df %>%
  mutate(cluster = jq_cluster) %>%
  group_by(cluster) %>%
  summarise_all(mean)
grouped_mushroom
```

Visualizamos la agrupación de los datos en función de los clusters obtenidos.

```{r}
jq_cluster <- factor(jq_cluster)

plot_ly(smaller_df,
  x = ~cap.diameter, y = ~stem.height,
  z = ~stem.width,
  color = ~jq_cluster
) %>%
  add_markers(size = 1.5)
```

Con el objetivo de comparar los resultados obtenidos en los dos algoritmos, vamos a calcular el rendimiento de cada uno de ellos, haciendo uso del accuracy como medida de bondad externa.

En primer lugar, calculamos el accuracy del modelo de k-means. Supondremos que la clase 1 es la clase "e" y la clase 2 es la clase "p".
Para ello, obtenemos las clases reales y las clases predichas, y calculamos el accuracy.

Volvemos a obtener el dataset reducido para poder tener las clases reales.
```{r}
smaller_df <- mushroom[split$Resample1, ]
```

```{r}
real_classes <- ifelse(smaller_df$class == "e", 1, 2)
predicted_classes <- km_sm_model$cluster
predicted_classes <- as.numeric(predicted_classes)
```

```{r}
accuracy <- sum(real_classes == predicted_classes) / length(real_classes)
print(accuracy)
```

Hacemos lo mismo con el modelo de clustering jerárquico, pero en este caso, supondremos que la clase 1 es la clase "p" y la clase 2 es la clase "e".
```{r}
real_classes <- ifelse(smaller_df$class == "e", 2, 1)
predicted_classes <- as.numeric(jq_cluster)
```

```{r}
accuracy <- sum(real_classes == predicted_classes) / length(real_classes)
print(accuracy)
```

Tras comparar los resultados obtenidos en los dos algoritmos, podemos afirmar que el modelo de clustering jerárquico ha obtenido un accuracy mayor para este dataset.
